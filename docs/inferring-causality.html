<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Inferring causality | An Introduction to Bayesian Cognitive Modelling in R</title>
  <meta name="description" content="5 Inferring causality | An Introduction to Bayesian Cognitive Modelling in R" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Inferring causality | An Introduction to Bayesian Cognitive Modelling in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Inferring causality | An Introduction to Bayesian Cognitive Modelling in R" />
  
  
  

<meta name="author" content="Fausto Carcassi" />


<meta name="date" content="2023-08-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rational-speech-acts.html"/>
<link rel="next" href="learning-in-a-language-of-thought.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#bits-and-pieces-of-r"><i class="fa fa-check"></i><b>1.1</b> Bits and pieces of R</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#a-motivating-example-sampling-from-an-urn"><i class="fa fa-check"></i><b>1.2</b> A motivating example: Sampling from an urn</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#sampling-possible-worlds-from-the-generative-model"><i class="fa fa-check"></i><b>1.2.1</b> Sampling possible worlds from the generative model</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#bayesian-update-learning-from-the-data"><i class="fa fa-check"></i><b>1.3</b> Bayesian update: Learning from the data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#reminder-bayes-theorem"><i class="fa fa-check"></i><b>1.3.1</b> Reminder: Bayes Theorem</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#applying-bayes-theorem-to-the-urn-case"><i class="fa fa-check"></i><b>1.3.2</b> Applying Bayes theorem to the urn case</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#implementation-detail-how-to-avoid-calculating-pd"><i class="fa fa-check"></i><b>1.3.3</b> Implementation detail: How to avoid calculating p(D)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#if-there-is-time-left"><i class="fa fa-check"></i><b>1.4</b> If there is time left…</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#answers"><i class="fa fa-check"></i><b>1.6</b> Answers</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="categorization.html"><a href="categorization.html"><i class="fa fa-check"></i><b>2</b> Categorization</a><ul>
<li class="chapter" data-level="2.1" data-path="categorization.html"><a href="categorization.html#the-case-of-a-single-observation"><i class="fa fa-check"></i><b>2.1</b> The case of a single observation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="categorization.html"><a href="categorization.html#little-r-programming-trick"><i class="fa fa-check"></i><b>2.1.1</b> Little R programming trick</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="categorization.html"><a href="categorization.html#the-case-of-multiple-observations"><i class="fa fa-check"></i><b>2.2</b> The case of multiple observations</a></li>
<li class="chapter" data-level="2.3" data-path="categorization.html"><a href="categorization.html#if-there-is-time-left-1"><i class="fa fa-check"></i><b>2.3</b> If there is time left…</a></li>
<li class="chapter" data-level="2.4" data-path="categorization.html"><a href="categorization.html#exercises-1"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
<li class="chapter" data-level="2.5" data-path="categorization.html"><a href="categorization.html#answers-1"><i class="fa fa-check"></i><b>2.5</b> Answers</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cultural-evolution.html"><a href="cultural-evolution.html"><i class="fa fa-check"></i><b>3</b> Cultural Evolution</a><ul>
<li class="chapter" data-level="3.1" data-path="cultural-evolution.html"><a href="cultural-evolution.html#the-languages"><i class="fa fa-check"></i><b>3.1</b> The languages</a></li>
<li class="chapter" data-level="3.2" data-path="cultural-evolution.html"><a href="cultural-evolution.html#bayesian-learning-of-the-language-from-data"><i class="fa fa-check"></i><b>3.2</b> Bayesian learning of the language from data</a></li>
<li class="chapter" data-level="3.3" data-path="cultural-evolution.html"><a href="cultural-evolution.html#cultural-evolution-with-bayesian-agents"><i class="fa fa-check"></i><b>3.3</b> Cultural evolution with Bayesian agents!</a></li>
<li class="chapter" data-level="3.4" data-path="cultural-evolution.html"><a href="cultural-evolution.html#if-there-is-time-left-2"><i class="fa fa-check"></i><b>3.4</b> If there is time left…</a></li>
<li class="chapter" data-level="3.5" data-path="cultural-evolution.html"><a href="cultural-evolution.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rational-speech-acts.html"><a href="rational-speech-acts.html"><i class="fa fa-check"></i><b>4</b> Rational Speech Acts</a><ul>
<li class="chapter" data-level="4.1" data-path="rational-speech-acts.html"><a href="rational-speech-acts.html#surprisal-in-information-theory"><i class="fa fa-check"></i><b>4.1</b> Surprisal in information theory</a></li>
<li class="chapter" data-level="4.2" data-path="rational-speech-acts.html"><a href="rational-speech-acts.html#the-rational-speech-act-model"><i class="fa fa-check"></i><b>4.2</b> The Rational Speech Act model</a></li>
<li class="chapter" data-level="4.3" data-path="rational-speech-acts.html"><a href="rational-speech-acts.html#if-there-is-time-left-3"><i class="fa fa-check"></i><b>4.3</b> If there is time left…</a></li>
<li class="chapter" data-level="4.4" data-path="rational-speech-acts.html"><a href="rational-speech-acts.html#exercises-3"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferring-causality.html"><a href="inferring-causality.html"><i class="fa fa-check"></i><b>5</b> Inferring causality</a><ul>
<li class="chapter" data-level="5.1" data-path="inferring-causality.html"><a href="inferring-causality.html#setup"><i class="fa fa-check"></i><b>5.1</b> Setup</a></li>
<li class="chapter" data-level="5.2" data-path="inferring-causality.html"><a href="inferring-causality.html#approximating-the-likelihood-with-a-simulation"><i class="fa fa-check"></i><b>5.2</b> Approximating the likelihood with a simulation</a></li>
<li class="chapter" data-level="5.3" data-path="inferring-causality.html"><a href="inferring-causality.html#bayesian-inference"><i class="fa fa-check"></i><b>5.3</b> Bayesian inference</a></li>
<li class="chapter" data-level="5.4" data-path="inferring-causality.html"><a href="inferring-causality.html#one-complication-from-real-participants"><i class="fa fa-check"></i><b>5.4</b> One complication from real participants</a></li>
<li class="chapter" data-level="5.5" data-path="inferring-causality.html"><a href="inferring-causality.html#putting-it-all-together"><i class="fa fa-check"></i><b>5.5</b> Putting it all together</a></li>
<li class="chapter" data-level="5.6" data-path="inferring-causality.html"><a href="inferring-causality.html#exercises-4"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
<li class="chapter" data-level="5.7" data-path="inferring-causality.html"><a href="inferring-causality.html#appendices-you-dont-need-to-understand-them"><i class="fa fa-check"></i><b>5.7</b> Appendices (You don’t need to understand them)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="inferring-causality.html"><a href="inferring-causality.html#appendix-1"><i class="fa fa-check"></i><b>5.7.1</b> Appendix 1</a></li>
<li class="chapter" data-level="5.7.2" data-path="inferring-causality.html"><a href="inferring-causality.html#appendix-2-gamma1."><i class="fa fa-check"></i><b>5.7.2</b> Appendix 2: <span class="math inline">\(\gamma=1\)</span>.</a></li>
<li class="chapter" data-level="5.7.3" data-path="inferring-causality.html"><a href="inferring-causality.html#appendix-3-another-complication"><i class="fa fa-check"></i><b>5.7.3</b> Appendix 3: Another complication</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html"><i class="fa fa-check"></i><b>6</b> Learning in a Language of Thought</a><ul>
<li class="chapter" data-level="6.1" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html#the-logsumexp-function"><i class="fa fa-check"></i><b>6.1</b> The LogSumExp function</a></li>
<li class="chapter" data-level="6.2" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html#the-conceptual-primitives-and-number-systems"><i class="fa fa-check"></i><b>6.2</b> The conceptual primitives and number systems</a></li>
<li class="chapter" data-level="6.3" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html#learning-an-lot"><i class="fa fa-check"></i><b>6.3</b> Learning an LoT</a></li>
<li class="chapter" data-level="6.4" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html#if-there-is-time-left-4"><i class="fa fa-check"></i><b>6.4</b> If there is time left…</a></li>
<li class="chapter" data-level="6.5" data-path="learning-in-a-language-of-thought.html"><a href="learning-in-a-language-of-thought.html#exercises-5"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Cognitive Modelling in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferring-causality" class="section level1 hasAnchor">
<h1><span class="header-section-number">5</span> Inferring causality<a href="inferring-causality.html#inferring-causality" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Reading: Steyvers, Mark, Joshua B. Tenenbaum, Eric-Jan Wagenmakers, and Ben Blum. “Inferring Causal Networks from Observations and Interventions.” Cognitive Science 27, no. 3 (May 2003): 453–89.</p>
<div id="setup" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.1</span> Setup<a href="inferring-causality.html#setup" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are three aliens, which we can call A, B, and C. <em>At each timestep</em>, a Bayesian learner makes one observation, which consists of one word for each alien. There are <span class="math inline">\(n\)</span> possible words. After a certain number of timesteps <span class="math inline">\(T\)</span> (i.e. after having made <span class="math inline">\(T\)</span>-many observations), the learner has to infer which alien is reading which alien’s mind based on the observed groups of words (which we will call ‘the data’ <span class="math inline">\(D\)</span>). There are two possible causal structures:</p>
<ul>
<li>CC (common cause model): A and C both read C’s mind. A &lt;- C -&gt; B. First, a random word is chosen for C. Then, A and B independently get the same word as C with probability <span class="math inline">\(\alpha\)</span>, and a random word (which could possibly be the same as C’s word, by chance) with probability <span class="math inline">\(1-\alpha\)</span>.</li>
<li>CE (common effect model): C can read both A’s and B’s minds. A -&gt; C &lt;- B. First, a random word is selected for A and a random word is selected for B. Then, C reads both A’s and B’s minds, each with independent probability of success <span class="math inline">\(\alpha\)</span>, i.e. with probability <span class="math inline">\(\alpha\)</span> they read the right word and with probability <span class="math inline">\(1-\alpha\)</span> they read the wrong word. If C reads both minds, it produces one of the two read words at random. If it fails to read both minds, it produces a random word.</li>
</ul>
<div class="figure">
<img src="DAGs.png" alt="The two hypotheses." />
<p class="caption">The two hypotheses.</p>
</div>
<p>In the model below <span class="math inline">\(\alpha=0.8\)</span> and <span class="math inline">\(n=10\)</span>.</p>
<p>There are 4 patterns with different probability, because the specific words don’t matter: all that matters is which aliens produce the same or different words. Here they are:</p>
<table>
<thead>
<tr class="header">
<th align="right">Index</th>
<th align="left">Case <span class="math inline">\(d\)</span></th>
<th align="left">Description</th>
<th align="left"><span class="math inline">\(P(d | CC)\)</span></th>
<th align="left"><span class="math inline">\(P(d|CE)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left"><span class="math inline">\(A=B=C\)</span></td>
<td align="left">All same</td>
<td align="left">0.67</td>
<td align="left">0.096</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left"><span class="math inline">\(A=C\)</span> and <span class="math inline">\(B != C\)</span> or <span class="math inline">\(B=C\)</span> and <span class="math inline">\(A != C\)</span></td>
<td align="left">Two connected same</td>
<td align="left">0.3</td>
<td align="left">0.87</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left"><span class="math inline">\(A=B\)</span> and <span class="math inline">\(A!= C\)</span></td>
<td align="left">Two disconnected same</td>
<td align="left">0.0036</td>
<td align="left">0.0036</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left"><span class="math inline">\(A!= B\)</span> and <span class="math inline">\(B != C\)</span> and <span class="math inline">\(A != C\)</span></td>
<td align="left">All different</td>
<td align="left">0.029</td>
<td align="left">0.029</td>
</tr>
</tbody>
</table>
<p>Draw them to get a sense of what the possible cases are and why they are different from each other.</p>
</div>
<div id="approximating-the-likelihood-with-a-simulation" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.2</span> Approximating the likelihood with a simulation<a href="inferring-causality.html#approximating-the-likelihood-with-a-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The probability of two words being the same by chance is 10/100, i.e. 0.1. How to get the probabilities above for each case and each hypothesis? Let’s consider an example: the first case (where <span class="math inline">\(A=B=C\)</span>) for hypothesis CC (A &lt;- C -&gt; B) can be obtained in four different ways. First, both A and B correctly read C’s mind (two events of probability 0.8 happen, so 0.8*0.8). Second, A correctly reads C’s mind and B just happens to get the same word (0.8*0.2*0.1). Third, B correctly reads C’s mind and A just happens to get the same word (0.8*0.2*0.1). Finally, neither A nor B correctly read C’s mind, but they both just happen to get the same word (0.2*0.1*0.2*0.1).</p>
<p>It is a bit laborious to determine the probabilities by enumerating the possible events. However, we can do it by sampling! I.e. we simulate a bunch of times what happens given an hypothesis and see what proportions of the samples have e.g. A=B=C, and that gives us an approximation of the probability of each observation given the hypothesis. Let’s write a function to sample under CE and under CC:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1">sample.bernoulli &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">p.true=</span><span class="fl">0.8</span>){</a>
<a class="sourceLine" id="cb105-2" title="2">  <span class="kw">sample</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="ot">TRUE</span>,<span class="ot">FALSE</span>),<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span><span class="kw">c</span>(p.true,<span class="dv">1</span><span class="op">-</span>p.true))</a>
<a class="sourceLine" id="cb105-3" title="3">}</a>
<a class="sourceLine" id="cb105-4" title="4"></a>
<a class="sourceLine" id="cb105-5" title="5">sample.word &lt;-<span class="st"> </span><span class="cf">function</span>(){</a>
<a class="sourceLine" id="cb105-6" title="6">  <span class="kw">sample</span>(<span class="dt">x=</span><span class="dv">10</span>,<span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb105-7" title="7">}</a>
<a class="sourceLine" id="cb105-8" title="8"></a>
<a class="sourceLine" id="cb105-9" title="9">produce.data.CE &lt;-<span class="st"> </span><span class="cf">function</span>(){</a>
<a class="sourceLine" id="cb105-10" title="10">  </a>
<a class="sourceLine" id="cb105-11" title="11">  word.A &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-12" title="12">  word.B &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-13" title="13">  </a>
<a class="sourceLine" id="cb105-14" title="14">  C.reads.A &lt;-<span class="st"> </span><span class="kw">sample.bernoulli</span>()</a>
<a class="sourceLine" id="cb105-15" title="15">  C.reads.B &lt;-<span class="st"> </span><span class="kw">sample.bernoulli</span>()</a>
<a class="sourceLine" id="cb105-16" title="16">  </a>
<a class="sourceLine" id="cb105-17" title="17">  <span class="cf">if</span> (C.reads.A <span class="op">&amp;</span><span class="st"> </span>C.reads.B){</a>
<a class="sourceLine" id="cb105-18" title="18">    <span class="co"># both</span></a>
<a class="sourceLine" id="cb105-19" title="19">    word.C &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(word.A,word.B),<span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb105-20" title="20">  } <span class="cf">else</span> <span class="cf">if</span> (C.reads.A){</a>
<a class="sourceLine" id="cb105-21" title="21">    <span class="co"># just A</span></a>
<a class="sourceLine" id="cb105-22" title="22">    word.C &lt;-<span class="st"> </span>word.A</a>
<a class="sourceLine" id="cb105-23" title="23">  } <span class="cf">else</span> <span class="cf">if</span> (C.reads.B){</a>
<a class="sourceLine" id="cb105-24" title="24">    <span class="co"># just B</span></a>
<a class="sourceLine" id="cb105-25" title="25">    word.C &lt;-<span class="st"> </span>word.B</a>
<a class="sourceLine" id="cb105-26" title="26">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb105-27" title="27">    <span class="co"># neither</span></a>
<a class="sourceLine" id="cb105-28" title="28">    word.C &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-29" title="29">  }</a>
<a class="sourceLine" id="cb105-30" title="30">  </a>
<a class="sourceLine" id="cb105-31" title="31">  <span class="kw">return</span>(<span class="kw">c</span>(word.A, word.B, word.C))</a>
<a class="sourceLine" id="cb105-32" title="32">}</a>
<a class="sourceLine" id="cb105-33" title="33"></a>
<a class="sourceLine" id="cb105-34" title="34">produce.data.CC &lt;-<span class="st"> </span><span class="cf">function</span>(){</a>
<a class="sourceLine" id="cb105-35" title="35">  </a>
<a class="sourceLine" id="cb105-36" title="36">  word.C &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-37" title="37">  </a>
<a class="sourceLine" id="cb105-38" title="38">  A.reads.C &lt;-<span class="st"> </span><span class="kw">sample.bernoulli</span>()</a>
<a class="sourceLine" id="cb105-39" title="39">  B.reads.C &lt;-<span class="st"> </span><span class="kw">sample.bernoulli</span>()</a>
<a class="sourceLine" id="cb105-40" title="40">  </a>
<a class="sourceLine" id="cb105-41" title="41">  <span class="cf">if</span> (A.reads.C){</a>
<a class="sourceLine" id="cb105-42" title="42">    word.A &lt;-<span class="st"> </span>word.C</a>
<a class="sourceLine" id="cb105-43" title="43">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb105-44" title="44">    word.A &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-45" title="45">  }</a>
<a class="sourceLine" id="cb105-46" title="46">  </a>
<a class="sourceLine" id="cb105-47" title="47">  <span class="cf">if</span> (B.reads.C){</a>
<a class="sourceLine" id="cb105-48" title="48">    word.B &lt;-<span class="st"> </span>word.C</a>
<a class="sourceLine" id="cb105-49" title="49">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb105-50" title="50">    word.B &lt;-<span class="st"> </span><span class="kw">sample.word</span>()</a>
<a class="sourceLine" id="cb105-51" title="51">  }</a>
<a class="sourceLine" id="cb105-52" title="52">  </a>
<a class="sourceLine" id="cb105-53" title="53">  <span class="kw">return</span>(<span class="kw">c</span>(word.A, word.B, word.C))</a>
<a class="sourceLine" id="cb105-54" title="54">}</a></code></pre></div>
<p>And let’s test this function:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" title="1"><span class="kw">produce.data.CE</span>()</a></code></pre></div>
<pre><code>## [1] 6 6 6</code></pre>
<p>What we need next is a function that take a specific datapoint produced by the function above and categorizes it into one of the four cases described above.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1">divide.cases &lt;-<span class="st"> </span><span class="cf">function</span>(samples){</a>
<a class="sourceLine" id="cb108-2" title="2">  </a>
<a class="sourceLine" id="cb108-3" title="3">  A.equals.B &lt;-<span class="st"> </span>samples[<span class="dv">1</span>,] <span class="op">==</span><span class="st"> </span>samples[<span class="dv">2</span>,]</a>
<a class="sourceLine" id="cb108-4" title="4">  A.equals.C &lt;-<span class="st"> </span>samples[<span class="dv">1</span>,] <span class="op">==</span><span class="st"> </span>samples[<span class="dv">3</span>,]</a>
<a class="sourceLine" id="cb108-5" title="5">  B.equals.C &lt;-<span class="st"> </span>samples[<span class="dv">2</span>,] <span class="op">==</span><span class="st"> </span>samples[<span class="dv">3</span>,]</a>
<a class="sourceLine" id="cb108-6" title="6">  </a>
<a class="sourceLine" id="cb108-7" title="7">  <span class="co"># the cases above</span></a>
<a class="sourceLine" id="cb108-8" title="8">  case<span class="fl">.1</span> &lt;-<span class="st"> </span>A.equals.B <span class="op">&amp;</span><span class="st"> </span>A.equals.C <span class="op">&amp;</span><span class="st"> </span>B.equals.C</a>
<a class="sourceLine" id="cb108-9" title="9">  case<span class="fl">.2</span> &lt;-<span class="st"> </span>(A.equals.C <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>B.equals.C) <span class="op">|</span><span class="st"> </span>(B.equals.C <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>A.equals.C)</a>
<a class="sourceLine" id="cb108-10" title="10">  case<span class="fl">.3</span> &lt;-<span class="st"> </span>A.equals.B <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>A.equals.C</a>
<a class="sourceLine" id="cb108-11" title="11">  case<span class="fl">.4</span> &lt;-<span class="st"> </span><span class="op">!</span>A.equals.B <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>B.equals.C <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>A.equals.C</a>
<a class="sourceLine" id="cb108-12" title="12">  </a>
<a class="sourceLine" id="cb108-13" title="13">  <span class="kw">return</span>(<span class="kw">rbind</span>(case<span class="fl">.1</span>,case<span class="fl">.2</span>,case<span class="fl">.3</span>,case<span class="fl">.4</span>))</a>
<a class="sourceLine" id="cb108-14" title="14">}</a></code></pre></div>
<p>Let’s approximate the likelihood by seeing how often the 4 cases happen under each of the two hypotheses:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" title="1">n=<span class="dv">500000</span></a>
<a class="sourceLine" id="cb109-2" title="2"></a>
<a class="sourceLine" id="cb109-3" title="3">samples.CC &lt;-</a>
<a class="sourceLine" id="cb109-4" title="4"><span class="st">  </span><span class="kw">rowMeans</span>(<span class="kw">divide.cases</span>(<span class="kw">replicate</span>(n, <span class="kw">produce.data.CC</span>())))</a>
<a class="sourceLine" id="cb109-5" title="5"></a>
<a class="sourceLine" id="cb109-6" title="6">samples.CE &lt;-</a>
<a class="sourceLine" id="cb109-7" title="7"><span class="st">  </span><span class="kw">rowMeans</span>(<span class="kw">divide.cases</span>(<span class="kw">replicate</span>(n, <span class="kw">produce.data.CE</span>())))</a>
<a class="sourceLine" id="cb109-8" title="8"></a>
<a class="sourceLine" id="cb109-9" title="9"><span class="kw">print</span>(samples.CC)</a></code></pre></div>
<pre><code>##   case.1   case.2   case.3   case.4 
## 0.672456 0.295436 0.003640 0.028468</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" title="1"><span class="kw">print</span>(samples.CE)</a></code></pre></div>
<pre><code>##   case.1   case.2   case.3   case.4 
## 0.096366 0.871172 0.003562 0.028900</code></pre>
<p>You can see that these approximations are pretty close to the ones in the paper!</p>
</div>
<div id="bayesian-inference" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.3</span> Bayesian inference<a href="inferring-causality.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Just to be extra clear,</p>
<ul>
<li>the hypotheses here are <span class="math inline">\(CC\)</span> and <span class="math inline">\(CE\)</span></li>
<li>the data <span class="math inline">\(D\)</span> is a series of observations <span class="math inline">\(d_1, d_2, ..., d_T\)</span>, where each <span class="math inline">\(d\)</span> consists of three words (well, in the code we use numbers instead of words because it’s easier).</li>
</ul>
<p>Now we have all the ingredients we need to do Bayesian update, namely the likelihood (the probability of a causal structure given a datapoint, which is described by the table above) <span class="math inline">\(P(\text{data }=D \mid \text{hypothesis }=CC)\)</span> and <span class="math inline">\(P(\text{data }=D \mid \text{hypothesis }=CE)\)</span>, and the prior over hypotheses (which we assume is uniform, so each of the two possible hypotheses gets 0.5) <span class="math inline">\(P(\text{hypothesis }=CC)\)</span> and <span class="math inline">\(P(\text{hypothesis }=CE)\)</span>. We calculate the posterior as usual:</p>
<p><span class="math display">\[
P(CC \mid D) = \frac{P(D \mid CC) P(CC)}{P(D \mid CC) P(CC) + P(D \mid CE) P(CE)}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P(CE \mid D) = \frac{P(D \mid CE) P(CE)}{P(D \mid CC) P(CC) + P(D \mid CE) P(CE)}
\]</span></p>
<p>There is one number that we can use in this model to guess which hypothesis is true, namely <span class="math inline">\(\phi\)</span>. This is the logarithm of the posterior over one hypothesis divided by the other, which is equal to:</p>
<p><span class="math display">\[
\phi = \log\frac{P(CC \mid D)}{P(CE \mid D)} \\
\]</span></p>
<p>The reason this quantity is important is the following:</p>
<p><em><em>When <span class="math inline">\(\phi\)</span> is greater than 0, the data favours hypothesis CC. The greater the <span class="math inline">\(\phi\)</span>, the more the data favors hypothesis CC. When <span class="math inline">\(\phi\)</span> is 0, the data is neutral between the two hypotheses. When <span class="math inline">\(\phi\)</span> is smaller than 0, the data favors CE. The more negative the <span class="math inline">\(\phi\)</span>, the more the data favours CE.</em></em></p>
<p>There is an equivalent, simpler way of calculating <span class="math inline">\(\phi\)</span> just by using the likelihood of the individual observations at each timestep <span class="math inline">\(t\)</span> up to the final timestep <span class="math inline">\(T\)</span> (assuming a uniform prior over hypotheses):</p>
<p><span class="math display">\[
\phi = \sum_{t=1}^{T} \log \frac{P(d_t \mid CC)}{P(d_t \mid CE)}
\]</span></p>
<p>Note that this sum really means:</p>
<p><span class="math display">\[
\log \frac{P(d_1 \mid CC)}{P(d_1 \mid CE)} + \log \frac{P(d_2 \mid CC)}{P(d_2 \mid CE)} + ... + \log \frac{P(d_T \mid CC)}{P(d_T \mid CE)}
\]</span></p>
<p>You don’t need to understand why the two are equivalent (see Appendix 1 if you’re curious).</p>
</div>
<div id="one-complication-from-real-participants" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.4</span> One complication from real participants<a href="inferring-causality.html#one-complication-from-real-participants" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Up until this point in the course, we have modelled mostly perfect Bayesian agents, namely agents that learn and behave conforming perfectly to Bayes’ theorem (although sometimes their generative model might not accurately described reality). However, this paper shows us an interesting new direction: if we are interested in modeling real human participants, we can assume a Bayesian learner and then add on top of it some transformations that approximate ‘irrationality’. This might be a bit confusing now but should become clearer in the rest of this section.</p>
<p>The type of ‘irrationality’ we will add to the model concerns the way the learner actually selects an hypothesis given their posterior probability after seeing the data. A ‘perfect’ Bayesian agent would simply select the hypothesis, between CC and CE, that has the highest posterior probability. This would mean selecting CC whenever <span class="math inline">\(\phi &gt; 0\)</span> and CE whenever <span class="math inline">\(\phi &lt;0\)</span> (Can you explain why?). However, real humans might not be so rational. Rather, in the model they might not be as sure. Therefore, we write <span class="math inline">\(P(\text{guessing }CC)\)</span> (the probability that they will choose the <span class="math inline">\(CC\)</span> hypothesis) as follows:</p>
<p><span class="math display">\[
P(CC) = \frac{1}{1+e^{-\gamma \phi}}
\]</span></p>
<p>This is a case of the famous <em>logistic function</em> <span class="math inline">\(\frac{1}{1+e^{-x}}\)</span>, where <span class="math inline">\(x\)</span> is set to <span class="math inline">\(\gamma\phi\)</span>. The logistic function can be used when you want to ‘squeeze’ a value below 0 or above 1 between 0 and 1.</p>
<p>Let’s plot this for different values of the parameters to get a feeling of the behaviour (with the probability of the data given CE fixed to 0.4):</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" title="1">p.function &lt;-<span class="st"> </span><span class="cf">function</span>(p.data.given.CC, p.data.given.CE, gamma.var){</a>
<a class="sourceLine" id="cb113-2" title="2">  phi.var &lt;-<span class="st"> </span><span class="kw">log</span>(p.data.given.CC<span class="op">/</span>p.data.given.CE)</a>
<a class="sourceLine" id="cb113-3" title="3">  value &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>gamma.var<span class="op">*</span>phi.var))</a>
<a class="sourceLine" id="cb113-4" title="4">  <span class="kw">return</span>(value)</a>
<a class="sourceLine" id="cb113-5" title="5">}</a>
<a class="sourceLine" id="cb113-6" title="6"></a>
<a class="sourceLine" id="cb113-7" title="7">values.of.gamma &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb113-8" title="8">col &lt;-<span class="st"> </span><span class="kw">heat.colors</span>(<span class="kw">length</span>(values.of.gamma), <span class="dt">rev=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb113-9" title="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(values.of.gamma)){</a>
<a class="sourceLine" id="cb113-10" title="10">  <span class="kw">curve</span>(</a>
<a class="sourceLine" id="cb113-11" title="11">    <span class="kw">p.function</span>(x, <span class="fl">0.4</span>, values.of.gamma[i]),</a>
<a class="sourceLine" id="cb113-12" title="12">    <span class="dt">from =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb113-13" title="13">    <span class="dt">to =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb113-14" title="14">    <span class="dt">col =</span> col[i],</a>
<a class="sourceLine" id="cb113-15" title="15">    <span class="dt">add =</span> i <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb113-16" title="16">    <span class="dt">xlab =</span> <span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb113-17" title="17">    <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span></a>
<a class="sourceLine" id="cb113-18" title="18">  )</a>
<a class="sourceLine" id="cb113-19" title="19">}</a>
<a class="sourceLine" id="cb113-20" title="20"></a>
<a class="sourceLine" id="cb113-21" title="21"><span class="kw">legend</span>(<span class="fl">0.8</span>, <span class="fl">0.5</span>, values.of.gamma, <span class="dt">col=</span>col, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">title=</span><span class="kw">expression</span>(gamma))</a>
<a class="sourceLine" id="cb113-22" title="22"></a>
<a class="sourceLine" id="cb113-23" title="23"><span class="kw">title</span>(</a>
<a class="sourceLine" id="cb113-24" title="24">  <span class="kw">expression</span>(<span class="st">&quot;Prob of picking hypothesis CC given D and &quot;</span><span class="op">*</span>gamma),</a>
<a class="sourceLine" id="cb113-25" title="25">  <span class="dt">xlab=</span><span class="st">&#39;P(CC|D)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;P(CC)&#39;</span></a>
<a class="sourceLine" id="cb113-26" title="26">)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>The plot shows for each possible posterior probability of <span class="math inline">\(CC\)</span> given the data, the probability of selecting <span class="math inline">\(CC\)</span> as the true hypothesis. A couple things to note about the plot:</p>
<ul>
<li>Across all values of <span class="math inline">\(\gamma\)</span>, all the lines agree on three points: 0 (<span class="math inline">\(P(CE|D)=1\)</span>), 0.5 (<span class="math inline">\(P(\text{guessing }CC)=0.5\)</span>), and 1 (<span class="math inline">\(P(CC|D)=0\)</span>). This means that according to this model of hypothesis selection you will always pick an hypothesis if it has probability 1, and if the two hypothesis are equally likely to have produced the data changing the value of <span class="math inline">\(\gamma\)</span> will not make you lean towards either hypothesis.</li>
<li>When <span class="math inline">\(\gamma\)</span> is close to 0, the data only makes a small impact on the probability of choosing one hypothesis or the other (unless the data completely excludes one or the other hypothesis). When <span class="math inline">\(\gamma\)</span> gets larger, even a small support for one or the other hypothesis means that the agent will likely choose it.</li>
</ul>
</div>
<div id="putting-it-all-together" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.5</span> Putting it all together<a href="inferring-causality.html#putting-it-all-together" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we have a full model of how participants infer the true causal model given some data. Let’s implement it. Let’s produce some data to train the participant. We have the function to produce data for CE above, but let’s write it for CC too here:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" title="1">n.datapoints &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb114-2" title="2"></a>
<a class="sourceLine" id="cb114-3" title="3"><span class="co"># we can use empirical proportions obtained above by sampling</span></a>
<a class="sourceLine" id="cb114-4" title="4"><span class="co"># this is a close enough approximation</span></a>
<a class="sourceLine" id="cb114-5" title="5">likelihoods.CC &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">t</span>(samples.CC))</a>
<a class="sourceLine" id="cb114-6" title="6">likelihoods.CE &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">t</span>(samples.CE))</a>
<a class="sourceLine" id="cb114-7" title="7"></a>
<a class="sourceLine" id="cb114-8" title="8"><span class="co"># produce n.datapoints datapoints from world where CC is true</span></a>
<a class="sourceLine" id="cb114-9" title="9">data &lt;-<span class="st"> </span><span class="kw">replicate</span>(n.datapoints,<span class="kw">produce.data.CC</span>())</a></code></pre></div>
<p>Then, we get the likelihoods of the produced datapoints under the hypotheses:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" title="1"><span class="co"># get the likelihoods of the observed datapoints under hypothesis CC</span></a>
<a class="sourceLine" id="cb115-2" title="2">likelihoods.data.CC &lt;-<span class="st"> </span><span class="kw">colSums</span>(<span class="kw">ifelse</span>(</a>
<a class="sourceLine" id="cb115-3" title="3">  <span class="kw">divide.cases</span>(data),</a>
<a class="sourceLine" id="cb115-4" title="4">  <span class="kw">replicate</span>(n.datapoints,likelihoods.CC),</a>
<a class="sourceLine" id="cb115-5" title="5">  <span class="fl">0.</span></a>
<a class="sourceLine" id="cb115-6" title="6">))</a>
<a class="sourceLine" id="cb115-7" title="7"></a>
<a class="sourceLine" id="cb115-8" title="8"><span class="co"># and same for CE, for the same datapoints</span></a>
<a class="sourceLine" id="cb115-9" title="9">likelihoods.data.CE &lt;-<span class="st"> </span><span class="kw">colSums</span>(<span class="kw">ifelse</span>(</a>
<a class="sourceLine" id="cb115-10" title="10">  <span class="kw">divide.cases</span>(data),</a>
<a class="sourceLine" id="cb115-11" title="11">  <span class="kw">replicate</span>(n.datapoints,likelihoods.CE),</a>
<a class="sourceLine" id="cb115-12" title="12">  <span class="fl">0.</span></a>
<a class="sourceLine" id="cb115-13" title="13">))</a></code></pre></div>
<p>Finally, we calculate <span class="math inline">\(\phi\)</span> at each timestep:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" title="1"><span class="co"># cumulative phi as observations are received</span></a>
<a class="sourceLine" id="cb116-2" title="2">phi &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">log</span>(likelihoods.data.CC<span class="op">/</span>likelihoods.data.CE))</a>
<a class="sourceLine" id="cb116-3" title="3"></a>
<a class="sourceLine" id="cb116-4" title="4"><span class="kw">plot</span>(phi)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>And from <span class="math inline">\(\phi\)</span> calculate the probability of accepting hypothesis CC at each timestep.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" title="1">gamma &lt;-<span class="st"> </span><span class="fl">0.05</span></a>
<a class="sourceLine" id="cb117-2" title="2">p.choosing.CC &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>gamma<span class="op">*</span>phi))</a>
<a class="sourceLine" id="cb117-3" title="3"><span class="kw">plot</span>(p.choosing.CC)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
</div>
<div id="exercises-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.6</span> Exercises<a href="inferring-causality.html#exercises-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Make CE the true hypothesis instead and plot <span class="math inline">\(\phi\)</span> over 100 timesteps.</li>
<li>How do different values of <span class="math inline">\(\alpha\)</span> affect the results? Please plot them!</li>
<li>What happens when <span class="math inline">\(\gamma\)</span> becomes 0?</li>
<li>What is the effect of increasing and decreasing the <span class="math inline">\(\gamma\)</span> parameter on the way <span class="math inline">\(\phi\)</span> changes over time?</li>
</ol>
</div>
<div id="appendices-you-dont-need-to-understand-them" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.7</span> Appendices (You don’t need to understand them)<a href="inferring-causality.html#appendices-you-dont-need-to-understand-them" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="appendix-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.7.1</span> Appendix 1<a href="inferring-causality.html#appendix-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align}
\phi
&amp;= \log\frac{P(CC \mid D)}{P(CE \mid D)} \\
&amp;= \log \left( \frac{\left( \frac{P(CC)P(D \mid CC)}{P(D)}\right) }{\left( \frac{P(CE)P(D \mid CE)}{P(D)}\right)} \right) \\
&amp; \textrm{Multiply numerator and denominator by $P(D)$:} \\
&amp;= \log \frac{P(CC)P(D \mid CC)}{P(CE)P(D \mid CE)} \\
&amp; \textrm{Go from log of product to sum of logs:} \\
&amp;= \log \frac{P(CC)}{P(CE)} + \log \frac{P(D \mid CC)}{P(D \mid CE)} \\
&amp; \textrm{Uniform prior over the two hypotheses, so first term becomes log of 1 (i.e., 0):} \\
&amp;= \log \frac{P(D \mid CC)}{P(D \mid CE)} \\
&amp; \textrm{Observations are independent:} \\
&amp;= \log \prod_{t=1}^{T} \frac{P(d_t \mid CC)}{P(d_t \mid CE)} \\
&amp; \textrm{Log transforms product into sum:} \\
&amp;= \sum_{t=1}^{T} \log \frac{P(d_t \mid CC)}{P(d_t \mid CE)} \\
\end{align}\]</span></p>
</div>
<div id="appendix-2-gamma1." class="section level3 hasAnchor">
<h3><span class="header-section-number">5.7.2</span> Appendix 2: <span class="math inline">\(\gamma=1\)</span>.<a href="inferring-causality.html#appendix-2-gamma1." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align}
P(CC) 
&amp;= \frac{1}{1+e^{-\gamma\log \frac{P(D \mid CC)}{P(D \mid CE)} }} \\
&amp;= \frac{1}{1+e^{\log \left( \left( \frac{P(D \mid CC)}{P(D \mid CE)} \right)^{-\gamma} \right) }} \\
&amp;= \frac{1}{1+e^{\log \left( \left( \frac{P(D \mid CE)}{P(D \mid CC)} \right)^\gamma \right) }} \\
&amp;= \frac{1}{1+ \left( \frac{P(D \mid CE)}{P(D \mid CC)} \right)^\gamma} \\
&amp;= \frac{1}{ 1+ \frac{P(D \mid CE)}{P(D \mid CC)} } \\
&amp;= \frac{1}{ \frac{P(D \mid CC)+P(D \mid CE)}{P(D \mid CC)}} \\
&amp;= \frac{P(D \mid CC)}{P(D \mid CC)+P(D \mid CE)} \\
&amp;= P(CC \mid D)
\end{align}\]</span></p>
</div>
<div id="appendix-3-another-complication" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.7.3</span> Appendix 3: Another complication<a href="inferring-causality.html#appendix-3-another-complication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second complication in the model is the Bayesian learner might put less emphasis on older observations, i.e. they might play a smaller role in determining the posterior distribution. Recall from above that:</p>
<p><span class="math display">\[
\phi = \sum_{t=1}^{T} \log \frac{P(d_t \mid CC)}{P(d_t \mid CE)}
\]</span></p>
<p>Now, if one of the terms of the sum is 0, then it plays no role in determining <span class="math inline">\(\phi\)</span>, and therefore in the resulting probability of accepting one hypothesis or the other. Note: the term of the sum is going to be 0 when the argument of the <span class="math inline">\(\log\)</span> (the fraction) is 1, i.e. when the two hypotheses are equally probable. This makes sense, since that’s what happens when the data doesn’t privilege one hypothesis over the other.</p>
<p>What we can do to simulate decreasing importance of information in the past is to ‘shrink’ the terms of the argument towards 0. The terms we shrink completely to 0 are equivalent to the agent disregarding the information contained in that observation. In particular, we want earlier observations to be shrunk more, as we are imagining that the participant will take more recent observations more into account.</p>
<p>What we need is to multiply each term of the sum with a function of <span class="math inline">\(t\)</span> that increases with increasing <span class="math inline">\(t\)</span>. Here’s one such function, where <span class="math inline">\(\delta\)</span> controls how fast the function shrinks towards zero when going towards the past:</p>
<p><span class="math display">\[
e^{-\frac{T-t}{\delta}}
\]</span></p>
<p>Let’s plot it to make sure it has the shape we want (where <span class="math inline">\(t\)</span> ranges from 1 to <span class="math inline">\(T\)</span> included):</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" title="1">T =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb118-2" title="2"></a>
<a class="sourceLine" id="cb118-3" title="3">time.discount.f =<span class="st"> </span><span class="cf">function</span>(t, T, delta){</a>
<a class="sourceLine" id="cb118-4" title="4">  <span class="kw">return</span>(<span class="kw">exp</span>(<span class="op">-</span>(T<span class="op">-</span>t)<span class="op">/</span>delta))</a>
<a class="sourceLine" id="cb118-5" title="5">}</a>
<a class="sourceLine" id="cb118-6" title="6"></a>
<a class="sourceLine" id="cb118-7" title="7">values.of.delta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb118-8" title="8">col &lt;-<span class="st"> </span><span class="kw">heat.colors</span>(<span class="kw">length</span>(values.of.delta), <span class="dt">rev=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb118-9" title="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(values.of.delta)){</a>
<a class="sourceLine" id="cb118-10" title="10">  <span class="kw">curve</span>(</a>
<a class="sourceLine" id="cb118-11" title="11">    <span class="kw">time.discount.f</span>(x, T, values.of.delta[i]),</a>
<a class="sourceLine" id="cb118-12" title="12">    <span class="dt">from =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb118-13" title="13">    <span class="dt">to =</span> T,</a>
<a class="sourceLine" id="cb118-14" title="14">    <span class="dt">col =</span> col[i],</a>
<a class="sourceLine" id="cb118-15" title="15">    <span class="dt">add =</span> i <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb118-16" title="16">    <span class="dt">xlab =</span> <span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb118-17" title="17">    <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span></a>
<a class="sourceLine" id="cb118-18" title="18">  )</a>
<a class="sourceLine" id="cb118-19" title="19">}</a>
<a class="sourceLine" id="cb118-20" title="20"></a>
<a class="sourceLine" id="cb118-21" title="21"><span class="kw">legend</span>(<span class="dv">1</span>, <span class="fl">1.</span>, values.of.delta, <span class="dt">col=</span>col, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">title=</span><span class="kw">expression</span>(delta))</a>
<a class="sourceLine" id="cb118-22" title="22"></a>
<a class="sourceLine" id="cb118-23" title="23"><span class="kw">title</span>(</a>
<a class="sourceLine" id="cb118-24" title="24">  <span class="kw">expression</span>(<span class="st">&quot;Discounting factor for term t with given &quot;</span><span class="op">*</span>delta),</a>
<a class="sourceLine" id="cb118-25" title="25">  <span class="dt">xlab=</span><span class="st">&#39;t&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Discounting factor&#39;</span></a>
<a class="sourceLine" id="cb118-26" title="26">)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p>When <span class="math inline">\(\delta=0\)</span>, everything but the <span class="math inline">\(T\)</span>th weight is 0. When <span class="math inline">\(\delta=\inf\)</span>, every weight is 1. Therefore, we can use <span class="math inline">\(\delta\)</span> to control the ‘amount’ of rationality.</p>
<p>Finally, the other complication. The plot shows, as the observations come in, the probability of choosing hypothesis CC. Since CC is the true hypothesis, you can see that the agent slowly becomes more and more convinced of it:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" title="1">gamma &lt;-<span class="st"> </span><span class="fl">0.05</span></a>
<a class="sourceLine" id="cb119-2" title="2">p.choosing.CC &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>gamma<span class="op">*</span>phi))</a>
<a class="sourceLine" id="cb119-3" title="3"><span class="kw">plot</span>(p.choosing.CC)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>Now let’s add the time discounting complication described above.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1">delta &lt;-<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb120-2" title="2"></a>
<a class="sourceLine" id="cb120-3" title="3">log.proportion &lt;-<span class="st"> </span><span class="kw">log</span>(likelihoods.data.CC<span class="op">/</span>likelihoods.data.CE)</a>
<a class="sourceLine" id="cb120-4" title="4">phi &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb120-5" title="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n.datapoints){</a>
<a class="sourceLine" id="cb120-6" title="6">  time.discount &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(</a>
<a class="sourceLine" id="cb120-7" title="7">    <span class="dv">1</span><span class="op">:</span>i,</a>
<a class="sourceLine" id="cb120-8" title="8">    time.discount.f,</a>
<a class="sourceLine" id="cb120-9" title="9">    <span class="dt">T=</span>i,</a>
<a class="sourceLine" id="cb120-10" title="10">    <span class="dt">delta=</span>delta</a>
<a class="sourceLine" id="cb120-11" title="11">  ))</a>
<a class="sourceLine" id="cb120-12" title="12">  new.phi &lt;-<span class="st"> </span><span class="kw">sum</span>(log.proportion[<span class="dv">1</span><span class="op">:</span>i] <span class="op">*</span><span class="st"> </span>time.discount)</a>
<a class="sourceLine" id="cb120-13" title="13">  phi &lt;-<span class="st"> </span><span class="kw">c</span>(phi,new.phi)</a>
<a class="sourceLine" id="cb120-14" title="14">}</a>
<a class="sourceLine" id="cb120-15" title="15"><span class="kw">plot</span>(phi)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rational-speech-acts.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="learning-in-a-language-of-thought.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
