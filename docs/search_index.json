[["index.html", "An Introduction to Bayesian Cognitive Modelling in R Preface", " An Introduction to Bayesian Cognitive Modelling in R Fausto Carcassi 2023-08-16 Preface In the Winter semester 2021, I taught an introductory cognitive modeling course at the University of Amsterdam for 3rd year students with little programming experience. The course contained a combination of lectures and labs. I couldn’t find a coherent set of labs for a course like this, so I thought I’d write it myself. This book contains those labs. It’s got plenty of typos &amp; many parts could be clearer, but this has to happen over multiple iterations of the course! I learned R in a rush to do this, so the code could be better in many ways, but here we are. The whole thing is in base R except for a single package in the last lab with LogSumExp, which is there mainly to teach students how to use functions from a package. Feedback very welcome. This course was written while working in Jakub Szymanik’s project, with funding from the European Research Council under the European Union’s Seventh Framework Programme (FP/2007–2013)/ERC Grant Agreement n. STG 716230 CoSaQ (Thanks Jakub for being so patient while I was teaching!) "],["introduction.html", "1 Introduction 1.1 Bits and pieces of R 1.2 A motivating example: Sampling from an urn 1.3 Bayesian update: Learning from the data 1.4 If there is time left… 1.5 Exercises 1.6 Answers", " 1 Introduction Reading: Chapter 4 and 5 from Kruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Edition 2. Boston: Academic Press. 1.1 Bits and pieces of R Some useful shortcuts for writing markdown: ctrl+shift+k: Render in html ctrl+alt+i: Insert r cell ctrl+shift+enter: Run current code chunk Various: A vector is a series of values of the same type in a given order. We’ll mostly use vectors of integers or floats. A matrix is a bunch of numbers arranged in a square (rows and columns). Important: we can refer to elements of a matrix by their index, which consists of two numbers. The first number indicates the row of the element, the second number indicates the column. You assign a value to a variable with &lt;-, you check equality with ==. Bits of R that we’re going to use (make sure you understand them): c: Combines values into a vector or list 1:10: The ‘:’ creates a vector from 1 to 10 rep: Replicates an element a specified number of times print: Prints something sample: Samples values from a vector with the given probabilities (given in the prob argument) head: Shows just the first few values of a vector, or the first few rows of a matrix hist: plots an histogram. Useful to get a sense for the distribution of samples. rbinom: Repeatedly samples from a binomial distribution. Arguments: rbinom (# observations, # trails/observation, probability of success ) paste: connects elements in a single string that can be printed. sum: sums all the values in a vector / matrix for (i in c(1:10)): Loops over i, with i taking values between 1 and 10 (included). barplot: Plots a barplot cbind: Takes a series of vectors, connects them into a matrix so that each vector is a column. colSums: Sums the columns of a matrix. 1.2 A motivating example: Sampling from an urn Imagine that you have an urn that has black and red balls inside. You can’t see inside the urn and you don’t know how many red and black balls there are, but you know that there is a total of 9 balls. You are interested in how many red balls there are (or, equivalently, how many black balls there are: since you know the total number, knowing one implies the other). The hypotheses are {0 red balls, 1 red ball, 2 red balls, …, 9 red balls }, one for each way that the unknown part of the world could be (the “world” here just means whatever bit of the world we want to model, in this case the urn’s content). Let’s call them respectively R0, R1, R2, etc. Since you don’t know which hypothesis is true, you have a case of subjective uncertainty (the world is in a certain way, but you don’t know which way it is). Therefore, it is natural to represent your uncertainty with a probability distribution over the possible unknown states that the world could be in, namely our 10 hypotheses: each hypothesis gets a probability, and the probabilities sum to 1. Which probabilities should you give to the hypotheses? Assume that you have no reason for thinking that there is any particular number of red or black balls in the urn. Then, it is natural to give each hypothesis the same probability 1/10 = 0.1, so that we have a uniform distribution over hypotheses (this is called the principle of indifference, which historically has been super important in the development of probability theory. The rabbit hole goes deep). This distribution over hypotheses (the possible ways the the unknown part of the world could be) before we observe any new data is called the prior. Suppose now you put your hand inside the urn, grab a ball at random, take it out of the urn, and look at it. The possible observations are { black ball, red ball }, call them \\(BB\\) and \\(RB\\) respectively. The probability of observing each color depends on which hypothesis is true, i.e. how many balls of each color are in the urn. For instance, if R0 is true (there are 0 red balls in the urn), then the probability of observing a black ball is 1 (\\(p(BB|R0)=1.\\)), and the probability of observing a red ball is 0 (\\(p(RB|R0)=0.\\)). The function of hypotheses that gives the probability of some specific observations given the hypothesis is called the likelihood (the likelihood is a bit confusing so don’t worry too much if you don’t get it now, going through examples will make it clearer). 1.2.1 Sampling possible worlds from the generative model Now we have a distribution over hypotheses, \\(p(H)\\), and a distribution over observations given each hypothesis, \\(p(D|H)\\). These two things together are called the generative model, because one thing we can do with them is generate possible scenarios, i.e. a combination of hypothesis and data sampled from that hypothesis. You can generate possible scenarios because prior and likelihood, if you multiply them together, define a joint distribution over hypotheses and observations: \\(p(D|H)p(H)=p(D,H)\\) (chain rule of probability). So a scenario is a sample from this joint distribution. How do we sample from the generative model? Since this joint is defined over each combination of \\(H\\) and \\(D\\), one option would be to sample directly from the joint distribution. However, there’s a simple way. Note that which hypothesis is true does not depend on the data, while the data depends on which hypothesis is true. Therefore, we can sample from the generative model as follows: Sample an hypothesis from the prior. Sample from the data given the hypothesis, using the likelihood function. Let’s first create a vector with the probability of each hypothesis: p.hypotheses &lt;- rep(0.1,10) print(p.hypotheses) ## [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Let’s use the generative model to generate possible worlds in the urn case. Let’s first create a vector of 10000 hypotheses sampled from the prior: prior.samples &lt;- sample(0:9, 10000, replace = TRUE, # the prob parameter allows to specify the probability # of each hypothesis. prob=p.hypotheses) head(prior.samples) ## [1] 1 1 3 7 2 4 Here each number corresponds to one hypothesis: 1 corresponds to R1, 2 to R2, etc. You can see each sample as a possible way the unknown part of the world could be. Since the prior was uniform (i.e. each hypothesis had the same probability), each hypothesis appears more or less equally often. Let’s plot this to make sure: hist( prior.samples, # this is to have the x ticks centered below the bars breaks=seq(min(prior.samples)-0.5, max(prior.samples)+0.5, by=1) ) Now for each element of prior_samples, we want to sample an observation. To do that, let’s first calculate the probability of the \\(RB\\) observation for each element of prior_samples. If an element of prior_samples is for instance 2, we want the probability of \\(RB\\) to be \\(2/9\\). In general, if the element is \\(n\\), we want the corresponding probability of \\(RB\\) to be \\(n/9\\). We can do this in a simple way with R, exploiting the fact that it intelligently adapts dimensions to each other: p.RB &lt;- prior.samples / 9 head(p.RB) ## [1] 0.1111111 0.1111111 0.3333333 0.7777778 0.2222222 0.4444444 Finally, we just need to sample one value for each element of p.RB: samples.RB &lt;- rbinom( n=length(p.RB), size=1, prob=p.RB ) head(samples.RB) ## [1] 0 1 1 1 0 0 samples.RB is equal to 1 when the observation was RB, and 0 when the observation was not RB (i.e. was BB). Just for sanity, let’s see how many observations of each type we got: hist( samples.RB, breaks=seq(min(samples.RB)-0.5, max(samples.RB)+0.5, by=1), xaxt=&#39;n&#39; ) axis(1,at=c(0,1)) Since we had a uniform probability over the hypotheses, we had the sample probability of sampling a black or a red ball across all hypotheses. Therefore, as expected, we got more or less the same number of black and red balls when sampling from the generative model. We have seen how to simulate possible states of the world from your prior+likelihood. However, suppose that we want to go the other way: you make some observations, and you want to update your distribution over hypotheses. In the urn case, suppose you pick one ball at random from the urn, and it’s black. What is the probability of each hypothesis given that observation (\\(p(H|BB)\\))? We can calculate it using Bayes theorem. 1.3 Bayesian update: Learning from the data 1.3.1 Reminder: Bayes Theorem As a reminder, this is Bayes theorem: \\[\\begin{align} \\color{violet}{p(H|D)} &amp;= \\frac{\\color{blue}{p(D|H)}\\color{green}{p(H)}}{\\color{red}{\\sum_{H_i} p(D|H_i)p(H_i)}} \\\\ &amp;= \\frac{\\color{blue}{p(D|H)}\\color{green}{p(H)}}{\\color{red}{p(D)}} \\end{align}\\] This mostly consists of expressions we have already encountered: \\(\\color{violet}{\\text{posterior}}\\), \\(\\color{blue}{\\text{likelihood}}\\), \\(\\color{green}{\\text{prior}}\\), and \\(\\color{red}{\\text{Bayesian evidence}}\\). Let’s go through them once more: The posterior is the probability of the hypothesis given the data. This is generally what we are interested in at the end: we see some data, we want to find the probability of each hypothesis given the data. The likelihood is the probability of the data given the hypothesis. The prior is the probability of the hypothesis prior to seeing any data. The evidence is the probability of the data across all hypotheses. Note that it does not depend on the hypothesis, and so the denominator is always the same. Effectively, it works as a normalization constant, in other words it makes sure that the sum of the posterior probability of all hypotheses is 1. The evidence is generally hard to calculate. Essentially, Bayes theorem is saying that the probability of a hypothesis given some data depends on (1) how likely the hypothesis was to generate the data and (2) how likely the hypothesis was before seeing the data. 1.3.2 Applying Bayes theorem to the urn case Suppose you are in the urn scenario above, with a uniform prior distribution over the 10 hypotheses. Now you pick a ball and it’s black. Given this observation of BB, how should you change the probabilities you give to each hypothesis? Intuitively, you should now give a little bit more probability to those hypotheses that have more black balls than red balls, because those are the hypotheses that make your observations more likely. Moreover, you can safely exclude hypothesis \\(R9\\), because your observation would be impossible if \\(R9\\) were true. Let’s calculate this with Bayes theorem. The prior is the vector p.hypotheses we defined above. Given that we have observed \\(BB\\), the likelihood should contain for each hypothesis the probability of the observation given the hypothesis. For \\(R0\\), the likelihood (\\(p(BB|R0)\\)) is 1. For \\(R1\\), 8 out of the 9 balls are black, and therefore \\(p(BB|R1)=8/9\\). For hypothesis \\(n\\), the corresponding likelihood is \\((9-n)/9\\). We can therefore encode the likelihood in the following vector: likelihood &lt;- (9-c(0:9)) / 9 print(likelihood) ## [1] 1.0000000 0.8888889 0.7777778 0.6666667 0.5555556 0.4444444 0.3333333 0.2222222 0.1111111 0.0000000 Now suppose we want to find the probability of hypothesis \\(R3\\) given your observation \\(BB\\). Let’s apply Bayes theorem: \\[ p(R3 \\mid BB) = \\frac{p(BB|R3)p(R3)}{\\sum_h p(BB|h)p(h)} \\] Let’s calculate the parts we need and use Bayes theorem: p.R3 &lt;- p.hypotheses[3] print(paste(&#39;p(R3)=&#39;, p.R3)) ## [1] &quot;p(R3)= 0.1&quot; p.BB.given.R3 &lt;- likelihood[3] print(paste(&#39;p(BB|R3)=&#39;, p.BB.given.R3)) ## [1] &quot;p(BB|R3)= 0.777777777777778&quot; evidence &lt;- sum(likelihood*p.hypotheses) print(paste(&#39;p(BB)=&#39;, p.BB.given.R3)) ## [1] &quot;p(BB)= 0.777777777777778&quot; p.R3.given.BB &lt;- p.R3 * p.BB.given.R3 / evidence print(paste(&#39;p(R3|BB)=&#39;, p.R3.given.BB)) ## [1] &quot;p(R3|BB)= 0.155555555555556&quot; Now let’s do the same for the other hypotheses, in a more compact way (note that the evidence is the same for all values): posterior &lt;- c() for (i in c(1:10)){ p.Ri.given.BB &lt;- p.hypotheses[i] * likelihood[i] / evidence print(paste(&#39;p(R&#39;, i, &#39;|BB)=&#39;, p.Ri.given.BB)) posterior &lt;- c(posterior, p.Ri.given.BB) } ## [1] &quot;p(R 1 |BB)= 0.2&quot; ## [1] &quot;p(R 2 |BB)= 0.177777777777778&quot; ## [1] &quot;p(R 3 |BB)= 0.155555555555556&quot; ## [1] &quot;p(R 4 |BB)= 0.133333333333333&quot; ## [1] &quot;p(R 5 |BB)= 0.111111111111111&quot; ## [1] &quot;p(R 6 |BB)= 0.0888888888888889&quot; ## [1] &quot;p(R 7 |BB)= 0.0666666666666667&quot; ## [1] &quot;p(R 8 |BB)= 0.0444444444444444&quot; ## [1] &quot;p(R 9 |BB)= 0.0222222222222222&quot; ## [1] &quot;p(R 10 |BB)= 0&quot; Note that, as expected, the posterior over hypotheses now is skewed towards those hypotheses with more black balls! Just for sanity, check that the posterior sums to 1: print(sum(posterior)) ## [1] 1 Finally, let’s plot the new posterior: barplot(posterior,space=0) axis(1,at=c(0:9)-0.5, labels=paste(&#39;R&#39;, c(0:9))) 1.3.3 Implementation detail: How to avoid calculating p(D) In practice, we generally do not need to calculate the evidence (denominator of Bayes rule) explicitly. When computing a posterior, we will mostly proceed as I am going to explain in this section, which is faster and also allows us to not worry about the denominator. First, we create a vector of prior probabilities, which has as many component as there are hypotheses. For instance, take the ten hypotheses above, so we can reuse p.hypotheses. Note that they sum to 1, as they should since it is a probability distribution. # don&#39;t worry too much about &#39;cat&#39; # if you&#39;re curious why not print, try # to use print(paste(&#39;Prior vector: &#39;, p.hypotheses)) instead cat(&#39;Prior vector: &#39;, p.hypotheses) ## Prior vector: 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Second, we create a likelihood array. When we did calculations above, we only had a vector with the likelihoods for the specific observation we did. However, we would like to have something that encodes the likelihood function for each possible observation given each possible hypothesis, rather than just for a specific observation. The likelihood defines for each hypothesis a distribution over possible observations. This is telling us given each possible way that the world could be, what is the probability of each observation. Suppose there are 2 possible observations as above, \\(BB\\) and \\(RB\\). Then, we can encode the likelihood as an array with shape (# hypotheses, # possible observations), or in this case (10, 2). The likelihood array in the urn case would be: likelihood.array &lt;- cbind( c(0:9)/9, 1-(c(0:9)/9) ) rownames(likelihood.array) &lt;- paste(&#39;R&#39;,c(0:9),sep=&#39;&#39;) colnames(likelihood.array) &lt;- c(&#39;RB&#39;, &#39;BB&#39;) print(likelihood.array) ## RB BB ## R0 0.0000000 1.0000000 ## R1 0.1111111 0.8888889 ## R2 0.2222222 0.7777778 ## R3 0.3333333 0.6666667 ## R4 0.4444444 0.5555556 ## R5 0.5555556 0.4444444 ## R6 0.6666667 0.3333333 ## R7 0.7777778 0.2222222 ## R8 0.8888889 0.1111111 ## R9 1.0000000 0.0000000 Second, we multiply the prior and likelihood vectors together (the nominator of Bayes theorem) element-wise (first element gets multiplied with first element, second element by second element, etc.). That gives us the nominator of Bayes theorem (prior times likelihood): bayes.numerator &lt;- likelihood.array * p.hypotheses print(bayes.numerator) ## RB BB ## R0 0.00000000 0.10000000 ## R1 0.01111111 0.08888889 ## R2 0.02222222 0.07777778 ## R3 0.03333333 0.06666667 ## R4 0.04444444 0.05555556 ## R5 0.05555556 0.04444444 ## R6 0.06666667 0.03333333 ## R7 0.07777778 0.02222222 ## R8 0.08888889 0.01111111 ## R9 0.10000000 0.00000000 Finally, note that we want a distribution for each column, i.e. a distribution over hypotheses given each observation. Therefore, we sum each column and then divide each element by the sum of its column: posterior &lt;- t(t(bayes.numerator)/colSums(bayes.numerator)) print(posterior) ## RB BB ## R0 0.00000000 0.20000000 ## R1 0.02222222 0.17777778 ## R2 0.04444444 0.15555556 ## R3 0.06666667 0.13333333 ## R4 0.08888889 0.11111111 ## R5 0.11111111 0.08888889 ## R6 0.13333333 0.06666667 ## R7 0.15555556 0.04444444 ## R8 0.17777778 0.02222222 ## R9 0.20000000 0.00000000 And that gives us the posterior without us having to explicitly calculate the evidence for each observation! 1.4 If there is time left… Calculate the posterior if your observation is \\(RB\\) instead of \\(BB\\). Calculate the posterior if you have two observations, namely \\(BB\\) and \\(RB\\). 1.5 Exercises Say that the probability that today is Sunday given that it’s sunny is written \\(p(\\text{today is Sunday} \\mid \\text{today is sunny})\\). Write expressions for the following: Probability that it rains given that the street is wet. Probability that the street is wet given that it rains. Probability that it rains and the street is wet. Suppose that the probability that John is bored is equal to the probability that John is bored given that he is watching a film. What does this imply about the two events (1) John is bored and (2) John is watching a film? Suppose there is a disease that affects 0.3% of the population, and a test that correctly identifies the disease 97% of the times (i.e. \\(p( \\text{test positive} \\mid \\text{has disease} )=0.97\\)). Given the probabilities above, can you infer the probability that you have the disease given that the test was positive? Given the probabilities above, can you infer the probability that you have the disease given that the test was negative? Given the probabilities above, can you infer the probability that you don’t have the disease given that the test was positive? Suppose that you had probability 0.4 for hypothesis R0 (in the lab notebook), and all other hypotheses had the same probability. Intuitively, what do you think will happen to the distribution of observations? Sample 1000 scenarios from the generative model and plot the results. 1.6 Answers \\(p(\\text{it rains} \\mid \\text{the street is wet})\\) \\(p(\\text{the street is wet} \\mid \\text{it rains})\\) \\(p(\\text{it rains &amp; the street is wet})\\) or \\(p(\\text{it rains, the street is wet})\\) or \\(p(\\text{it rains and the street is wet})\\) It implies that the event John is bored and the event John is watching a film are statistically independent of each other. Formally, \\(p(\\text{John is bored} \\mid \\text{John is watching a film}) = p(\\text{John is bored})\\) and \\(p(\\text{John is watching a film} \\mid \\text{John is bored}) = p(\\text{John is watching a film})\\). This does not imply that either event has happened or is happening, and it does not imply that the two events are equally probable. The solution would be simply an application of Bayes theorem: \\[ P(\\text{disease} \\mid \\text{positive}) = \\frac{P(\\text{positive} \\mid \\text{disease})P(\\text{disease})} {P(\\text{positive} \\mid \\text{disease})P(\\text{disease}) + P(\\text{positive} \\mid \\text{no disease})P(\\text{no disease})} \\] You have \\(P(\\text{no disease})=0.7\\), \\(P(\\text{disease}) = 0.3\\), \\(P(\\text{positive} \\mid \\text{disease})=0.97\\), and \\(P(\\text{negative} \\mid \\text{disease}) = 0.03\\). What you don’t have is \\(P(\\text{positive} \\mid \\text{no disease})\\). Therefore, one acceptable answer was: This cannot be calculated with the information given in the exercise. This exercise was to some extent a trick question, but given that nobody answered it this way I am going to assume that you are not used to this type of question and I won’t ask one again. Another acceptable answer was to assume that \\(P(\\text{positive} \\mid \\text{no disease})=0\\). In that case: \\[ P(\\text{disease} \\mid \\text{positive}) = \\frac{0.97 \\times 0.3}{0.97 \\times 0.3 + 0 \\times 0.7} = 1. \\] This makes intuitive sense: under this assumption the test is positive only when there is a disease, so if it is positive there is a disease. This is asking for \\(P(\\text{disease} \\mid \\text{negative}) = \\frac{P(\\text{negative} \\mid \\text{disease})P(\\text{disease})}{P(\\text{negative} \\mid \\text{disease})P(\\text{disease}) + P(\\text{negative} \\mid \\text{no disease})P(\\text{no disease})}\\). The answer is that you cannot calculate it, because you don’t have the necessary bits for applying Bayes theorem. Again, I accepted answers that assumed \\(P(\\text{positive} \\mid \\text{no disease})=0\\), in which case \\(P(\\text{negative} \\mid \\text{no disease})=1\\), and so: \\[ P(\\text{disease} \\mid \\text{negative}) = \\frac{0.03 \\times 0.3}{0.03 \\times 0.3 + 1 * 0.7} = \\frac{0.009}{1.009} \\approx 0.009 \\] \\(P(\\text{no disease} \\mid \\text{negative}) = 1 - P(\\text{disease} \\mid \\text{negative})\\). Since you can’t calculate \\(P(\\text{disease} \\mid \\text{negative})\\), you can’t answer this question. Again, under the assumptions that \\(P(\\text{positive} \\mid \\text{no disease})=0\\), \\(P(\\text{no disease} \\mid \\text{negative}) \\approx 0.999\\). R1 will have a higher probability than the other observations, while all the other observations will have the same probability as each other. Since R1 has 0 red balls, there is going to be overall more blue balls than red balls in the sampled observations. Following code: # only change is here p.hypotheses &lt;- c(0.4, rep(0.6/9,9)) # Same code as above prior.samples &lt;- sample(0:9, 10000, replace = TRUE, # the prob parameter allows to specify the probability # of each hypothesis. prob=p.hypotheses) p.RB &lt;- prior.samples / 9 samples.RB &lt;- rbinom( n=length(p.RB), size=1, prob=p.RB ) hist( samples.RB, breaks=seq(min(samples.RB)-0.5, max(samples.RB)+0.5, by=1), xaxt=&#39;n&#39; ) axis(1,at=c(0,1)) "],["categorization.html", "2 Categorization 2.1 The case of a single observation 2.2 The case of multiple observations 2.3 If there is time left… 2.4 Exercises 2.5 Answers", " 2 Categorization Reading: Tenenbaum, Joshua B, and Thomas L Griffiths. 2001. “Generalization, Similarity, and Bayesian Inference.” Behavioural and Brain Sciences, 629–40. In this lab, we are going to implement the categorization model from Tenenbaum &amp; Griffiths (2001). This is a great model because it combines conceptually simple and plausible ingredients to get sophisticated predictions for an important cognitive phenomenon, namely categorization. As it happens, it is also quite easy to implement (compared to other models!). This week I am not going to give you the meaning of each function we used, because figuring out what a function does is a skill you need to have anyway. Tip: beside googling (Google is a programmer’s best friend), you can ask R directly to tell you more about a function by running help(function.name). 2.1 The case of a single observation As always, when doing Bayesian modelling the first step is to describe the situation that is being modelled (i.e. specifying a generative model). We have seen what the generative model for the categorization case looks like in the lecture on Monday, so we can get onto the implementation directly (though maybe have the paper open to you can refer back to it if something it confusing). Let’s say our space is constrained between 1 and 100. n.stimuli &lt;- 100 Let’s define some vectors containing the information for each hypothesis. First, an array with the lower and upper bounds (inclusive) for each category. The shape of this borders array is (# categories, 2). Note that there is a total of 5050 categories. # all combinations of numbers from 1 up to (and including) the n.stimuli. borders &lt;- expand.grid(c(1:n.stimuli), c(1:n.stimuli)) # change row names # lb for &#39;lower bound&#39; and ub for &#39;upper bound&#39; colnames(borders) &lt;- c(&#39;lb&#39;, &#39;ub&#39;) # only keep those categories where the lower bound is lower # or equal to the upper bound borders &lt;- borders[borders[,1]&lt;=borders[,2],] # reset the row names row.names(borders) &lt;- c(1:nrow(borders)) head(borders) ## lb ub ## 1 1 1 ## 2 1 2 ## 3 2 2 ## 4 1 3 ## 5 2 3 ## 6 3 3 Second, an array with the size of each category: sizes &lt;- as.vector(t(borders[&#39;ub&#39;]-borders[&#39;lb&#39;] + 1)) head(sizes) ## [1] 1 2 1 3 2 1 Next, suppose we make a single observation, namely 60 (like in figure 1 from Tenenbaum &amp; Griffiths 2001). observation &lt;- 60 Let’s find the likelihood, i.e. the probability of the observation (60) given each category. This is described by strong sampling in the way we have seen in the Monday lecture. likelihood.given.observation &lt;- as.array(t(ifelse( (borders[&#39;lb&#39;] &lt;= observation) &amp; (borders[&#39;ub&#39;] &gt;= observation), 1/sizes, 0 ))) dim(likelihood.given.observation) ## [1] 1 5050 To find the posterior over hypotheses, assuming we have a uniform prior, it is enough to normalize the likelihood vector (can you see why?): posterior &lt;- (likelihood.given.observation / sum(likelihood.given.observation)) print(posterior[0:10]) ## [1] 0 0 0 0 0 0 0 0 0 0 print(sum(posterior)) ## [1] 1 Finally, let’s average across categories. For each point in the space (i.e. integers between 1 and 100 inclusive) sum the probabilities of the categories that include that point. To to this, we first create an array that has the whole posterior for each stimulus. # repeat the posterior for each stim, # to get a matrix with shape (stimulus, categories) tiled.posterior &lt;- matrix( posterior, nrow = n.stimuli, # number of stimuli ncol = length(posterior), byrow = TRUE ) Then, we create a mask so that for each stimulus we know which categories we need to consider in the sum (namely, those that contain the stimulus). # for each stimulus, get a Boolean mask of the categories # that contain the stimulus. The mask also has shape (stimulus, categories) stimulus.index &lt;- matrix(c(1:n.stimuli), nrow = n.stimuli, ncol = length(posterior), byrow = FALSE) # NOTE 1: we have to reshape the lb and ub vectors # so that it has the same shape as the mask # NOTE 2: borders[&#39;lb&#39;] returns a named vector, # while borders[[&#39;lb&#39;]] returns the vector of numbers mask &lt;- ( (stimulus.index &gt;= t(replicate(n.stimuli, borders[[&#39;lb&#39;]]))) &amp; (stimulus.index &lt;= t(replicate(n.stimuli, borders[[&#39;ub&#39;]]))) ) Finally, we sum the tiled posterior as indicated by the mask. In other words, for each stimulus we sum the posterior probability of those hypotheses that contain the posterior. We plot the resulting distribution, which has for each point the probability that the point belongs to the unknown category (the black dot is the observation). # use the Boolean array to attribut 0 to the hypotheses # that do not contain a point tiled.posterior[!mask] &lt;- 0 dim(tiled.posterior) ## [1] 100 5050 # sum across categories to get the probability of each point p.point &lt;- rowSums(tiled.posterior) plot(p.point) points(observation,0,pch=16) 2.1.1 Little R programming trick The operation above for doing binary comparison between an array with each row of a matrix can be rewritten more simply with the ‘sweep’ function in the following way: # basically, sweep applies an operation to each ith row (or column) of a matrix # using the ith element of an array greater.equal.to.lb &lt;- sweep(stimulus.index, 2, borders[[&#39;lb&#39;]], &#39;&gt;=&#39;) # check that they are the same for every element print(all((stimulus.index &gt;= t( replicate(n.stimuli, borders[[&#39;lb&#39;]]) )) == greater.equal.to.lb)) ## [1] TRUE # and therefore mask.with.sweep &lt;- ( sweep(stimulus.index, 2, borders[[&#39;lb&#39;]], &#39;&gt;=&#39;) &amp; sweep(stimulus.index, 2, borders[[&#39;ub&#39;]], &#39;&lt;=&#39;) ) all(mask==mask.with.sweep) ## [1] TRUE 2.2 The case of multiple observations In the case of multiple observations, only two things need to be changed: We need to compare the lower bound of the category with the lowest observation and the upper bound of the category with the greatest observation, rather than comparing them just with a single observation. The likelihood is not anymore just \\(\\frac{1}{\\text{size of category}}\\), but rather \\(\\frac{1}{\\text{size of category}^{\\text{N of observations}}}\\). observations &lt;- c(30, 50, 60) n.observations &lt;- length(observations) min.obs &lt;- min(observations) max.obs &lt;- max(observations) likelihood.given.observation &lt;- as.array(t(ifelse( # HERE IS ONE CHANGE (borders[&#39;lb&#39;] &lt;= min.obs) &amp; (borders[&#39;ub&#39;] &gt;= max.obs), # HERE IS ANOTHER CHANGE 1/(sizes^n.observations), 0 ))) posterior &lt;- (likelihood.given.observation / sum(likelihood.given.observation)) tiled.posterior &lt;- matrix( posterior, nrow = n.stimuli, # number of stimuli ncol = length(posterior), byrow = TRUE ) stimulus.index &lt;- matrix(c(1:n.stimuli), nrow = n.stimuli, ncol = length(posterior), byrow = FALSE) mask &lt;- ( sweep(stimulus.index, 2, borders[[&#39;lb&#39;]], &#39;&gt;=&#39;) &amp; sweep(stimulus.index, 2, borders[[&#39;ub&#39;]], &#39;&lt;=&#39;) ) tiled.posterior[!mask] &lt;- 0 p.point &lt;- rowSums(tiled.posterior) plot(p.point) points(observations,rep(0,length(observations)),pch=16) 2.3 If there is time left… Try to expand the model to a 2-d space, so that the hypotheses are rectangles instead of segments. 2.4 Exercises Imagine you have two observations at 100 and 110. Now, you get another observation between them at 105. What happens to the generalization gradient? Why? (NOTE: I don’t mean the exact numbers, but rather how the ‘shape’ of the generalization gradient changes. This doesn’t depend on the size of the space very much, but if it helps you to use a specific space assume that the space is between 0 and 200.) Plot the prior distribution describing the probability that each stimulus belongs to the true category before making any observation (NOTE: Use the space above with points between 1 and 100). Change the code so that there are 200 instead of 100 stimuli. Plot the probability of each stimulus belonging to the category with observations {100}, {150}, {100,150}. Now imagine that you have prior reason for thinking that the true category contains an even number of stimuli, so that any category with an even number of stimuli is twice as likely a priori as any category with an odd number of stimuli. Change the code to calculate the prior accordingly and plot the generalization function before observing any stimulus. Plot the generalization function after observing {50}, {85}, and {50, 85}. 2.5 Answers # Let&#39;s define three useful functions that we can reuse below, for clarity # NOTE: you didn&#39;t need to use functions, you could just copy-paste # the relevant bits of code each time. # NOTE: I have only made minimal changes to these function compared to the # code above to make it a bit more general. I have marked the changes with comments. # just copied some code from above. No changes here create.borders &lt;- function(n.stimuli){ borders &lt;- expand.grid(c(1:n.stimuli), c(1:n.stimuli)) colnames(borders) &lt;- c(&#39;lb&#39;, &#39;ub&#39;) borders &lt;- borders[borders[,1]&lt;=borders[,2],] row.names(borders) &lt;- c(1:nrow(borders)) # Only added a return line because it&#39;s a function return(borders) } # Use same code as above to calculate sizes create.sizes &lt;- function(borders){ return(as.vector(t(borders[&#39;ub&#39;]-borders[&#39;lb&#39;] + 1))) } # let&#39;s copy the bit for going from a distribution over categories to # the generalization gradient and put it into a function # (I just changed the word &#39;posterior&#39; with &#39;distribution&#39; for clarity) plot.gradient.from.categories.dist &lt;- function(distribution){ tiled.distribution &lt;- matrix( distribution, nrow = n.stimuli, # number of stimuli ncol = length(distribution), byrow = TRUE ) stimulus.index &lt;- matrix(c(1:n.stimuli), nrow = n.stimuli, ncol = length(distribution), byrow = FALSE) mask &lt;- ( sweep(stimulus.index, 2, borders[[&#39;lb&#39;]], &#39;&gt;=&#39;) &amp; sweep(stimulus.index, 2, borders[[&#39;ub&#39;]], &#39;&lt;=&#39;) ) tiled.distribution[!mask] &lt;- 0 p.point &lt;- rowSums(tiled.distribution) plot(p.point) } # copy the code for calculating the posterior # NOTE: only change is that we are also multiplying by the prior now # NOTE: if prior isn&#39;t specified, a uniform prior is assumed in this function calculate.posterior &lt;- function(observations, borders, sizes, prior=1){ n.observations &lt;- length(observations) min.obs &lt;- min(observations) max.obs &lt;- max(observations) likelihood.given.observation &lt;- as.array(t(ifelse( (borders[&#39;lb&#39;] &lt;= min.obs) &amp; (borders[&#39;ub&#39;] &gt;= max.obs), 1/(sizes^n.observations), 0 ))) # Only change here: multiply by prior posterior &lt;- (likelihood.given.observation*prior / sum(likelihood.given.observation*prior)) return(posterior) } The answer is that adding a new observation between two observations that we know are already examples of the category makes the generalization gradient decrease steeper as it gets further from the maximum and minimum observations. This is because under the strong sampling assumption, if the category had been much larger than the category between the observed minimum and maximum, it would have been likely for the new observation to be below the minimum or above the maximum of the previously observed examples. However, the same does not hold under weak sampling. To answer this question it was enough to change line 153 (observations &lt;- c(30, 50, 60)) to observations &lt;- c() and re-run the cell. n.stimuli &lt;- 100 borders &lt;- create.borders(n.stimuli) sizes &lt;- create.sizes(borders) posterior.exercise.1 &lt;- calculate.posterior(c(), borders, sizes) ## Warning in min(observations): no non-missing arguments to min; returning Inf ## Warning in max(observations): no non-missing arguments to max; returning -Inf plot.gradient.from.categories.dist(posterior.exercise.1) Alternatively, you could avoid calculating the posterior at all and just use the prior. prior &lt;- rep(1,length(sizes)) / length(sizes) plot.gradient.from.categories.dist(prior) n.stimuli &lt;- 200 borders &lt;- create.borders(n.stimuli) sizes &lt;- create.sizes(borders) posterior.exercise.3 &lt;- calculate.posterior(c(100), borders, sizes) plot.gradient.from.categories.dist(posterior.exercise.3) posterior.exercise.3 &lt;- calculate.posterior(c(150), borders, sizes) plot.gradient.from.categories.dist(posterior.exercise.3) posterior.exercise.3 &lt;- calculate.posterior(c(100, 150), borders, sizes) plot.gradient.from.categories.dist(posterior.exercise.3) # I accepted both a space between 1 and 100 and between 1 and 200 n.stimuli &lt;- 100 borders &lt;- create.borders(n.stimuli) sizes &lt;- create.sizes(borders) # create new prior even.prior &lt;- -(sizes%%2-2) even.prior &lt;- even.prior / sum(even.prior) distribution.exercise.4 &lt;- calculate.posterior(c(), borders, sizes, prior=even.prior) ## Warning in min(observations): no non-missing arguments to min; returning Inf ## Warning in max(observations): no non-missing arguments to max; returning -Inf plot.gradient.from.categories.dist(distribution.exercise.4) posterior.exercise.5 &lt;- calculate.posterior(c(50), borders, sizes, prior=even.prior) plot.gradient.from.categories.dist(posterior.exercise.5) posterior.exercise.5 &lt;- calculate.posterior(c(85), borders, sizes, prior=even.prior) plot.gradient.from.categories.dist(posterior.exercise.5) posterior.exercise.5 &lt;- calculate.posterior(c(50, 85), borders, sizes, prior=even.prior) plot.gradient.from.categories.dist(posterior.exercise.5) "],["cultural-evolution.html", "3 Cultural Evolution 3.1 The languages 3.2 Bayesian learning of the language from data 3.3 Cultural evolution with Bayesian agents! 3.4 If there is time left… 3.5 Exercises", " 3 Cultural Evolution Reading: Kirby, Simon, Mike Dowman, and Thomas L Grifﬁths. “Innateness and Culture in the Evolution of Language.” PNAS 104, no. 12 (2007): 5241–45. https://doi.org/10/c78969. This lab will be about implementing a simple model of cultural evolution of language. 3.1 The languages The first thing we need to do is create the set of all languages we are going to consider. Each language will be modelled as a set of signals, where each signal refers to some (and possibly all) or none of the objects. For instance, one signal might refer to two of three objects, another to none of the objects, and yet another to all of the objects. We do not want to consider all possible languages, because we want the agent to always have at least one signal to send for each possible object. Therefore, we will exclude those languages such that there is no signal for one or more of the objects. We can represent a language as a 3 by 3 matrix. Each row represents one signal and each column is one object, so there are three signals and three objects. This is an example of a language: # Each language is a 3 by 3 matrix. l1 &lt;- matrix( c(0,1,1, 1,0,0, 1,1,0), nrow=3 ) print(l1) ## [,1] [,2] [,3] ## [1,] 0 1 1 ## [2,] 1 0 1 ## [3,] 1 0 0 Note that a language is in a sense simply a list of 9 numbers (0s and 1s) organized in a grid. So the simplest way to create the set of all languages is to generate the set of all possible lists of nine 0s and 1s, and then organize each of them in a grid. Let’s start by creating all combinations: flat.languages &lt;- as.matrix(expand.grid(rep(list(c(0,1)),9))) head(flat.languages) ## Var1 Var2 Var3 Var4 Var5 Var6 Var7 Var8 Var9 ## [1,] 0 0 0 0 0 0 0 0 0 ## [2,] 1 0 0 0 0 0 0 0 0 ## [3,] 0 1 0 0 0 0 0 0 0 ## [4,] 1 1 0 0 0 0 0 0 0 ## [5,] 0 0 1 0 0 0 0 0 0 ## [6,] 1 0 1 0 0 0 0 0 0 Now we can restructure each of those lists into a 3 by 3 matrix. What we get is a three-dimensional array: the first index represents the language, the second index the signal, and the third index the object. So if you take a specific language from the list (e.g. languages.all.combinations[9,,]) you get a 3 by 3 matrix. # reshape all combinations of 9 0s and 1s into 3 by 3 grids # new array has shape (language, word, meaning) languages.all.combinations &lt;- array(flat.languages, dim=c(512,3,3)) # print an example of a language languages.all.combinations[2,,] ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 Let’s do a sanity check! Since we are considering all possible languages, across all languages there should be no difference between any combination of word and object. colSums(languages.all.combinations) ## [,1] [,2] [,3] ## [1,] 256 256 256 ## [2,] 256 256 256 ## [3,] 256 256 256 Now let’s find those languages where there are states for which there is no word. We are not going to consider those languages! number.signals.per.object &lt;- colSums(aperm(languages.all.combinations, c(2,1,3))) indices.to.include &lt;- apply(number.signals.per.object != 0,1, all) # create a new language array with only those languages that have # at least one word for each state languages &lt;- languages.all.combinations[indices.to.include,,] # let&#39;s also keep track of the number of signals per object # of just the languages we consider # (it&#39;ll be useful later) number.signals.per.object.languages &lt;- number.signals.per.object[indices.to.include,] Another sanity check: go through a few languages and make sure that they indeed have at least one word for each object. There are 343 of them! languages[1,,] ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 0 0 0 ## [3,] 0 0 0 We are left with 343 languages that satisfy our condition (namely, that there is at least one word for each object, so that whatever the agents observe they’ll have something to say). Next, we can write a function that given a language, produces an object (given by the world) and selects a signal at random in the language among the ones that refer to the object. pick.object &lt;- function(language){ object &lt;- sample(x=1:3, size=1) # get the *indices* of the signals that apply to the object in the language indices.signals &lt;- which(language[,object]==1) # pick one of the indices signal &lt;- sample(indices.signals, size=1) return(c(object, signal)) } Another sanity check! Run this a bunch of times with different languages and make sure you understand what is going on. language.index &lt;- 10 print(languages[language.index,,]) ## [,1] [,2] [,3] ## [1,] 1 0 1 ## [2,] 1 1 0 ## [3,] 0 0 0 print(pick.object(languages[language.index,,])) ## [1] 3 1 3.2 Bayesian learning of the language from data Now we have a function to produce a single combination of object and signal given a language. Let’s write a function that does Bayesian updating: going from the list of all languages, a prior, and some data to a posterior over languages. bayesian.update &lt;- function(object, signal, prior, languages){ # the likelihood (probability of that combination of object and signal given a language) # for a given language is equal to 0 if the signal is not compatible with the object # in the language, and otherwise 1/(number of signals compatible with the object in the language) # the number of signals compatible with the object in each language # is in the number.signals.per.object which we used before! test &lt;- languages[,signal,object]==0 no &lt;- 1/number.signals.per.object.languages[,object] yes &lt;- 0 likelihood &lt;- ifelse( test, yes, no ) # calculate a normalized posterior unnormalized.posterior &lt;- likelihood * prior posterior &lt;- unnormalized.posterior / sum(unnormalized.posterior) return(posterior) } We have written the function to do the Bayesian updating, let’s test it on real data: language.index &lt;- 10 print(&#39;language:&#39;) ## [1] &quot;language:&quot; print(languages[language.index,,]) ## [,1] [,2] [,3] ## [1,] 1 0 1 ## [2,] 1 1 0 ## [3,] 0 0 0 # define a uniform prior # that we will use again later on multiple times uniform.prior &lt;- rep(1,nrow(languages)) / nrow(languages) language.index &lt;- 20 language.sample &lt;- pick.object(languages[language.index,,]) object &lt;- language.sample[1] signal &lt;- language.sample[2] posterior &lt;- bayesian.update(object, signal, uniform.prior, languages) barplot(posterior) Now we’d like to have the agent learn on more than one observation! We could do it in clever complicated ways, but let’s keep it simple. We can just use a loop where we keep producing data from the language, and then updating our beliefs after each observation. run.multiple.observations &lt;- function(n.observations, language, prior){ # Okay, we could have directly have &#39;current.beliefs&#39; as the parameter # but I am adding this line for you for clarity posterior &lt;- prior # loop over the observations for (i in 1:n.observations){ prior &lt;- posterior # produce one datapoint (i.e. one combination of object and signal) new.data &lt;- pick.object(language) object &lt;- new.data[1] signal &lt;- new.data[2] # do bayesian update on the new data # and set &#39;current beliefs&#39; to the new posterior # NOTE: we are using the previous beliefs as the prior here posterior &lt;- bayesian.update(object, signal, prior, languages) } return(posterior) } # update belief with 10 observations current.beliefs &lt;- run.multiple.observations(10, languages[10,,], uniform.prior) barplot(current.beliefs) 3.3 Cultural evolution with Bayesian agents! Finally, we have all the ingredients necessary to run an iterated learning experiment. iterate &lt;- function(n.iterations, n.observations, prior){ # start with an agent with a random language current.language.index &lt;- sample(1:nrow(languages),size=1) history &lt;- c(current.language.index) for (i in 1:n.iterations){ # first calculate the learner&#39;s posterior given the data produced by the # previous generation&#39;s teacher posterior &lt;- run.multiple.observations( n.observations, languages[current.language.index,,], prior ) # then select a language given a posterior # either by picking the language with the greatest probability # or sampling from the posterior # here we sample current.language.index &lt;- sample(nrow(languages), size=1, prob=posterior) history &lt;- c(history, current.language.index) } return(history) } n.iterations &lt;- 1000 n.observations &lt;- 1 history &lt;- iterate(n.iterations, n.observations, uniform.prior) head(history) ## [1] 279 23 198 170 98 155 If you run for 10k generations, you will see that it starts being pretty uniform over the set of languages! How can you interpret that? barplot(history) 3.4 If there is time left… Instead of having a single agent in each generation, implement multiple agents. The cultural child for each cultural parent is picked at random from the whole population, so that one cultural parent can have multiple cultural children (otherwise the model would be equivalent to multiple independent IL chains). Add a selection by communicative pressure: decide whether an agent gets to be part of the following generation depending on how successfully it can comunicate with its own cultural parent. 3.5 Exercises Exercise 1: What is the language that will lead to the worst communication and why? What are the language(s) that will lead to the best communication and why? Exercise 2: Run the iterated learning model for 10000 generations, but setting the n.observations parameter to 1 and then to 50 (this will take a while to run!). What changes in the results? Can you explain this difference? Exercise 3: The model above has sample agents, which sample the language from their posterior. How would you implement maximum-a-posteriori agents, which always select the language with the highest posterior probability? In the model above, we have used a uniform prior over the languages. But assume that for some reason you think that signal 1 fits particularly well with object 1. So a language where signal 1 refers to object 1 is going to have a higher prior probability than it would otherwise. Exercise 4: Write the code to produce a prior that is consistent with the description above. There are various ways of doing it, but basically any language where signal 1 refers to object 1 should have a higher prior probability than any language where signal 1 does not refer to object 1. Exercise 5: Run the iterated learning with this prior and plot the distribution over language over 10k generations. What do you notice compared to a uniform prior? "],["rational-speech-acts.html", "4 Rational Speech Acts 4.1 Surprisal in information theory 4.2 The Rational Speech Act model 4.3 If there is time left… 4.4 Exercises", " 4 Rational Speech Acts Reading: Sections 1 and 2 from: Scontras, Gregory, Michael Henry Tessler, and Michael Franke. 2021. “A Practical Introduction to the Rational Speech Act Modeling Framework.” 4.1 Surprisal in information theory Information theory is a big field of research. However, here we will only use one (although very important) concepts from information theory, namely surprisal. Surprisal is a quantity that describes, intuitively, how unexpected a certain observation is. The unexpectedness of an observation is intuitively a function of the probability of that observation: the more probable the observation, the less unexpected it will be, and the more improbable the more unexpected. How to formalize unexpectedness as a function of probability, i.e. find a function \\(f\\) so that given a probability \\(p\\) we can calculate its level of unexpectedness \\(f(p)\\)? We can put some constraints on \\(f\\) and then find an actual function that satisfies them: When an event has probability 0, it is infinitely unexpected. In formal terms, \\(f(0.)=\\infty\\). When an event has probability 1, it is not unexpected at all. In formal terms, \\(f(1.)=0.\\) Say we have two independent events with probabilities \\(p_1\\) and \\(p_2\\), so that the probability of both of them occurring is \\(p_1 p_2\\). We want the total unexpectedness of observing both events to be simply the sum of the unexpectedness of the two events. In formal terms \\(f(p_1 p_2) = f(p_1)+f(p_2)\\). Given these three requirements, you can see that the following is the function we are looking for to describe surprisal (unexpectedness) as a function of probability: \\[ f(p) = -\\log(p) \\] Check that the three requirements above are satisfied: \\(-\\log(p)\\) tends to \\(\\infty\\) as \\(p\\) goes to 0. \\(-\\log(1)=0\\). \\(-\\log(p_1 p_2)=-(\\log(p_1)+\\log(p_2))=(-\\log(p_1))+(-\\log(p_2))\\). Here’s the plot of this function: curve(-log(x), from=0, to=1) Why is surprisal important to us? Because it can work as a guide to what signal to send to a listener. Imagine that you observe a number between 1 and 10, and you need to describe it to a listener, who hears your description and then has to pick a number - you win the game if number you observed is the same as the number picked by the participant. Suppose you observe 2. Now, would you rather say ‘I saw an even number’ or ‘I saw 2’? Clearly the latter is better when it comes to make the listener guess which number you observed. Why? Well, you can think about it in terms of surprisal: you are trying to minimize the listener’s surprisal of the true number given the signal. In other words, you are trying to send a signal such that the number you observed is not surprising for the listener after they received your signal. Exercise: Calculate the surprisal of ‘I saw an even number’ and ‘I saw two’ (given that you saw 2). 4.2 The Rational Speech Act model Let’s start by defining some useful functions: rowNormalize &lt;- function(x){ # first create a vector with the sums of the rows summed.rows &lt;- apply(x,1,&#39;sum&#39;) # then divide each element by the sum of its row normalized.rows &lt;- sweep(x, 1, summed.rows, &#39;/&#39;) # at the end you get a probability distribution for each row return(normalized.rows) } colNormalize &lt;- function(x){ # same as above, but with columns summed.columns &lt;- apply(x,2,&#39;sum&#39;) normalized.columns &lt;- sweep(x, 2, summed.columns, &#39;/&#39;) # at the end you get a probability distribution for each column return(normalized.columns) } softMax &lt;- function(x, alpha){ # first we calculate the numerator of softmax # (see definition) unnormalized.softmax &lt;- exp(alpha*x) # then we normalize each column normalized.softmax &lt;- colNormalize(unnormalized.softmax) return(normalized.softmax) } Then define the language. Each row corresponds to a signal and each column to a state. (The example I have in mind is: there’s two cookies in a jar, and you are describing to your friend how many you had. You can say ‘I had some’, ‘I had all’, ‘I had none’, or stay silent. ‘I had some’ would mean that you had 1 or 2 cookies, ‘I had all’ would mean that you had 2 cookies, etc. If you stay silent, that’s compatible with all states - you’re not excluding any option) language &lt;- rbind( c(0,1,1), c(0,0,1), c(1,0,0), c(1,1,1) ) rownames(language) &lt;- c(&#39;some&#39;, &#39;all&#39;, &#39;none&#39;, &#39;silence&#39;) colnames(language) &lt;- c(0,1,2) heatmap(language, Colv=NA, Rowv = NA, scale=&#39;none&#39;) Now, let’s calculate the literal listener. The listener defines a probability vector for every row (i.e. every signal), because they get a signal and calculate a distribution over states. The literal listener assumes that given a state a random one of the signals compatible with the state is chosen. Since the literal listener has a uniform distribution over states, the posterior can be obtained simply by normalizing the language by row. l0 &lt;- rowNormalize(language) heatmap(l0, Colv=NA, Rowv = NA) Next, let’s calculate the pragmatic speaker. The pragmatic speaker observes a state and has to choose a signal, therefore we’ll end up with a matrix where each column is a distribution. However, the pragmatic speaker does not simply pick a random one among the signals compatible with the observed state. Rather, they tend to pick a signal that has the greatest utility for the literal listener, where utility is calculated as the negative surprisal, \\(-(-\\log(p))\\), of the state given the signal from the point of view of the literal listener. s1 &lt;- softMax(log(l0), 2.) heatmap(s1, Colv=NA, Rowv = NA) Finally, we can calculate the pragmatic listener. This is similar to the literal listener above in that it receives a signal and calculates a distribution over states. However, unlike the literal listener above, it knows the signal isn’t simply selected at random. Rather, it imagines the signal as being selected by the pragmatic speaker. l1 &lt;- rowNormalize(s1) heatmap(l1, Colv=NA, Rowv = NA) Note that the pragmatic listener correctly infers a scalar implicature. Although ‘most’ is semantically compatible with the state where ‘all’ is also true, in general the pragmatic listener will infer that if ‘most’ was uttered, ‘all’ was not true (or the pragmatic speaker would have said ‘all’ instead). This is a bit tricky to wrap your head around, so make sure you have followed every step of the simulation above and see how it corresponds to your linguistic intuition. 4.3 If there is time left… Add more meanings and signals and see how the model behaves. Add costs to the utterances based on the equations in the paper, and observed what effects it has. Rewrite the code above to work with log probabilities. 4.4 Exercises What is the surprisal of observing: 4 heads in a row from an unbiased coin? 2 heads and 2 tails? 3 red balls from a cup containing 3 blue balls and 1 red ball? What is the effect of increasing the rationality parameter in the RSA model? What happens when the rationality parameter goes to \\(\\infty\\)? And 0? Add a cost to the signals as described in the paper, so that silence has cost 0 and the other three signals cost 1. What is the effect of increasing the cost of a signal on the probability of the signal being uttered? Add one level of recursion to the RSA code. What happens to the \\(p(2|\\text{all})\\)? Plot it. What happens as the number of levels increases? "],["inferring-causality.html", "5 Inferring causality 5.1 Setup 5.2 Approximating the likelihood with a simulation 5.3 Bayesian inference 5.4 One complication from real participants 5.5 Putting it all together 5.6 Exercises 5.7 Appendices (You don’t need to understand them)", " 5 Inferring causality Reading: Steyvers, Mark, Joshua B. Tenenbaum, Eric-Jan Wagenmakers, and Ben Blum. “Inferring Causal Networks from Observations and Interventions.” Cognitive Science 27, no. 3 (May 2003): 453–89. 5.1 Setup There are three aliens, which we can call A, B, and C. At each timestep, a Bayesian learner makes one observation, which consists of one word for each alien. There are \\(n\\) possible words. After a certain number of timesteps \\(T\\) (i.e. after having made \\(T\\)-many observations), the learner has to infer which alien is reading which alien’s mind based on the observed groups of words (which we will call ‘the data’ \\(D\\)). There are two possible causal structures: CC (common cause model): A and C both read C’s mind. A &lt;- C -&gt; B. First, a random word is chosen for C. Then, A and B independently get the same word as C with probability \\(\\alpha\\), and a random word (which could possibly be the same as C’s word, by chance) with probability \\(1-\\alpha\\). CE (common effect model): C can read both A’s and B’s minds. A -&gt; C &lt;- B. First, a random word is selected for A and a random word is selected for B. Then, C reads both A’s and B’s minds, each with independent probability of success \\(\\alpha\\), i.e. with probability \\(\\alpha\\) they read the right word and with probability \\(1-\\alpha\\) they read the wrong word. If C reads both minds, it produces one of the two read words at random. If it fails to read both minds, it produces a random word. The two hypotheses. In the model below \\(\\alpha=0.8\\) and \\(n=10\\). There are 4 patterns with different probability, because the specific words don’t matter: all that matters is which aliens produce the same or different words. Here they are: Index Case \\(d\\) Description \\(P(d | CC)\\) \\(P(d|CE)\\) 1 \\(A=B=C\\) All same 0.67 0.096 2 \\(A=C\\) and \\(B != C\\) or \\(B=C\\) and \\(A != C\\) Two connected same 0.3 0.87 3 \\(A=B\\) and \\(A!= C\\) Two disconnected same 0.0036 0.0036 4 \\(A!= B\\) and \\(B != C\\) and \\(A != C\\) All different 0.029 0.029 Draw them to get a sense of what the possible cases are and why they are different from each other. 5.2 Approximating the likelihood with a simulation The probability of two words being the same by chance is 10/100, i.e. 0.1. How to get the probabilities above for each case and each hypothesis? Let’s consider an example: the first case (where \\(A=B=C\\)) for hypothesis CC (A &lt;- C -&gt; B) can be obtained in four different ways. First, both A and B correctly read C’s mind (two events of probability 0.8 happen, so 0.8*0.8). Second, A correctly reads C’s mind and B just happens to get the same word (0.8*0.2*0.1). Third, B correctly reads C’s mind and A just happens to get the same word (0.8*0.2*0.1). Finally, neither A nor B correctly read C’s mind, but they both just happen to get the same word (0.2*0.1*0.2*0.1). It is a bit laborious to determine the probabilities by enumerating the possible events. However, we can do it by sampling! I.e. we simulate a bunch of times what happens given an hypothesis and see what proportions of the samples have e.g. A=B=C, and that gives us an approximation of the probability of each observation given the hypothesis. Let’s write a function to sample under CE and under CC: sample.bernoulli &lt;- function(p.true=0.8){ sample(x=c(TRUE,FALSE),size=1,prob=c(p.true,1-p.true)) } sample.word &lt;- function(){ sample(x=10,size=1) } produce.data.CE &lt;- function(){ word.A &lt;- sample.word() word.B &lt;- sample.word() C.reads.A &lt;- sample.bernoulli() C.reads.B &lt;- sample.bernoulli() if (C.reads.A &amp; C.reads.B){ # both word.C &lt;- sample(c(word.A,word.B),size=1) } else if (C.reads.A){ # just A word.C &lt;- word.A } else if (C.reads.B){ # just B word.C &lt;- word.B } else { # neither word.C &lt;- sample.word() } return(c(word.A, word.B, word.C)) } produce.data.CC &lt;- function(){ word.C &lt;- sample.word() A.reads.C &lt;- sample.bernoulli() B.reads.C &lt;- sample.bernoulli() if (A.reads.C){ word.A &lt;- word.C } else { word.A &lt;- sample.word() } if (B.reads.C){ word.B &lt;- word.C } else { word.B &lt;- sample.word() } return(c(word.A, word.B, word.C)) } And let’s test this function: produce.data.CE() ## [1] 9 4 4 What we need next is a function that take a specific datapoint produced by the function above and categorizes it into one of the four cases described above. divide.cases &lt;- function(samples){ A.equals.B &lt;- samples[1,] == samples[2,] A.equals.C &lt;- samples[1,] == samples[3,] B.equals.C &lt;- samples[2,] == samples[3,] # the cases above case.1 &lt;- A.equals.B &amp; A.equals.C &amp; B.equals.C case.2 &lt;- (A.equals.C &amp; !B.equals.C) | (B.equals.C &amp; !A.equals.C) case.3 &lt;- A.equals.B &amp; !A.equals.C case.4 &lt;- !A.equals.B &amp; !B.equals.C &amp; !A.equals.C return(rbind(case.1,case.2,case.3,case.4)) } Let’s approximate the likelihood by seeing how often the 4 cases happen under each of the two hypotheses: n=500000 samples.CC &lt;- rowMeans(divide.cases(replicate(n, produce.data.CC()))) samples.CE &lt;- rowMeans(divide.cases(replicate(n, produce.data.CE()))) print(samples.CC) ## case.1 case.2 case.3 case.4 ## 0.672780 0.295102 0.003568 0.028550 print(samples.CE) ## case.1 case.2 case.3 case.4 ## 0.095748 0.871482 0.003546 0.029224 You can see that these approximations are pretty close to the ones in the paper! 5.3 Bayesian inference Just to be extra clear, the hypotheses here are \\(CC\\) and \\(CE\\) the data \\(D\\) is a series of observations \\(d_1, d_2, ..., d_T\\), where each \\(d\\) consists of three words (well, in the code we use numbers instead of words because it’s easier). Now we have all the ingredients we need to do Bayesian update, namely the likelihood (the probability of a causal structure given a datapoint, which is described by the table above) \\(P(\\text{data }=D \\mid \\text{hypothesis }=CC)\\) and \\(P(\\text{data }=D \\mid \\text{hypothesis }=CE)\\), and the prior over hypotheses (which we assume is uniform, so each of the two possible hypotheses gets 0.5) \\(P(\\text{hypothesis }=CC)\\) and \\(P(\\text{hypothesis }=CE)\\). We calculate the posterior as usual: \\[ P(CC \\mid D) = \\frac{P(D \\mid CC) P(CC)}{P(D \\mid CC) P(CC) + P(D \\mid CE) P(CE)} \\] and \\[ P(CE \\mid D) = \\frac{P(D \\mid CE) P(CE)}{P(D \\mid CC) P(CC) + P(D \\mid CE) P(CE)} \\] There is one number that we can use in this model to guess which hypothesis is true, namely \\(\\phi\\). This is the logarithm of the posterior over one hypothesis divided by the other, which is equal to: \\[ \\phi = \\log\\frac{P(CC \\mid D)}{P(CE \\mid D)} \\\\ \\] The reason this quantity is important is the following: When \\(\\phi\\) is greater than 0, the data favours hypothesis CC. The greater the \\(\\phi\\), the more the data favors hypothesis CC. When \\(\\phi\\) is 0, the data is neutral between the two hypotheses. When \\(\\phi\\) is smaller than 0, the data favors CE. The more negative the \\(\\phi\\), the more the data favours CE. There is an equivalent, simpler way of calculating \\(\\phi\\) just by using the likelihood of the individual observations at each timestep \\(t\\) up to the final timestep \\(T\\) (assuming a uniform prior over hypotheses): \\[ \\phi = \\sum_{t=1}^{T} \\log \\frac{P(d_t \\mid CC)}{P(d_t \\mid CE)} \\] Note that this sum really means: \\[ \\log \\frac{P(d_1 \\mid CC)}{P(d_1 \\mid CE)} + \\log \\frac{P(d_2 \\mid CC)}{P(d_2 \\mid CE)} + ... + \\log \\frac{P(d_T \\mid CC)}{P(d_T \\mid CE)} \\] You don’t need to understand why the two are equivalent (see Appendix 1 if you’re curious). 5.4 One complication from real participants Up until this point in the course, we have modelled mostly perfect Bayesian agents, namely agents that learn and behave conforming perfectly to Bayes’ theorem (although sometimes their generative model might not accurately described reality). However, this paper shows us an interesting new direction: if we are interested in modeling real human participants, we can assume a Bayesian learner and then add on top of it some transformations that approximate ‘irrationality’. This might be a bit confusing now but should become clearer in the rest of this section. The type of ‘irrationality’ we will add to the model concerns the way the learner actually selects an hypothesis given their posterior probability after seeing the data. A ‘perfect’ Bayesian agent would simply select the hypothesis, between CC and CE, that has the highest posterior probability. This would mean selecting CC whenever \\(\\phi &gt; 0\\) and CE whenever \\(\\phi &lt;0\\) (Can you explain why?). However, real humans might not be so rational. Rather, in the model they might not be as sure. Therefore, we write \\(P(\\text{guessing }CC)\\) (the probability that they will choose the \\(CC\\) hypothesis) as follows: \\[ P(CC) = \\frac{1}{1+e^{-\\gamma \\phi}} \\] This is a case of the famous logistic function \\(\\frac{1}{1+e^{-x}}\\), where \\(x\\) is set to \\(\\gamma\\phi\\). The logistic function can be used when you want to ‘squeeze’ a value below 0 or above 1 between 0 and 1. Let’s plot this for different values of the parameters to get a feeling of the behaviour (with the probability of the data given CE fixed to 0.4): p.function &lt;- function(p.data.given.CC, p.data.given.CE, gamma.var){ phi.var &lt;- log(p.data.given.CC/p.data.given.CE) value &lt;- 1/(1+exp(-gamma.var*phi.var)) return(value) } values.of.gamma &lt;- c(0.1, 0.5, 1, 2, 4) col &lt;- heat.colors(length(values.of.gamma), rev=TRUE) for (i in seq_along(values.of.gamma)){ curve( p.function(x, 0.4, values.of.gamma[i]), from = 0, to = 1, col = col[i], add = i != 1, xlab = &#39;&#39;, ylab = &#39;&#39; ) } legend(0.8, 0.5, values.of.gamma, col=col, lty=1, title=expression(gamma)) title( expression(&quot;Prob of picking hypothesis CC given D and &quot;*gamma), xlab=&#39;P(CC|D)&#39;, ylab=&#39;P(CC)&#39; ) The plot shows for each possible posterior probability of \\(CC\\) given the data, the probability of selecting \\(CC\\) as the true hypothesis. A couple things to note about the plot: Across all values of \\(\\gamma\\), all the lines agree on three points: 0 (\\(P(CE|D)=1\\)), 0.5 (\\(P(\\text{guessing }CC)=0.5\\)), and 1 (\\(P(CC|D)=0\\)). This means that according to this model of hypothesis selection you will always pick an hypothesis if it has probability 1, and if the two hypothesis are equally likely to have produced the data changing the value of \\(\\gamma\\) will not make you lean towards either hypothesis. When \\(\\gamma\\) is close to 0, the data only makes a small impact on the probability of choosing one hypothesis or the other (unless the data completely excludes one or the other hypothesis). When \\(\\gamma\\) gets larger, even a small support for one or the other hypothesis means that the agent will likely choose it. 5.5 Putting it all together Now we have a full model of how participants infer the true causal model given some data. Let’s implement it. Let’s produce some data to train the participant. We have the function to produce data for CE above, but let’s write it for CC too here: n.datapoints &lt;- 100 # we can use empirical proportions obtained above by sampling # this is a close enough approximation likelihoods.CC &lt;- t(t(samples.CC)) likelihoods.CE &lt;- t(t(samples.CE)) # produce n.datapoints datapoints from world where CC is true data &lt;- replicate(n.datapoints,produce.data.CC()) Then, we get the likelihoods of the produced datapoints under the hypotheses: # get the likelihoods of the observed datapoints under hypothesis CC likelihoods.data.CC &lt;- colSums(ifelse( divide.cases(data), replicate(n.datapoints,likelihoods.CC), 0. )) # and same for CE, for the same datapoints likelihoods.data.CE &lt;- colSums(ifelse( divide.cases(data), replicate(n.datapoints,likelihoods.CE), 0. )) Finally, we calculate \\(\\phi\\) at each timestep: # cumulative phi as observations are received phi &lt;- cumsum(log(likelihoods.data.CC/likelihoods.data.CE)) plot(phi) And from \\(\\phi\\) calculate the probability of accepting hypothesis CC at each timestep. gamma &lt;- 0.05 p.choosing.CC &lt;- 1/(1+exp(-gamma*phi)) plot(p.choosing.CC) 5.6 Exercises Make CE the true hypothesis instead and plot \\(\\phi\\) over 100 timesteps. How do different values of \\(\\alpha\\) affect the results? Please plot them! What happens when \\(\\gamma\\) becomes 0? What is the effect of increasing and decreasing the \\(\\gamma\\) parameter on the way \\(\\phi\\) changes over time? 5.7 Appendices (You don’t need to understand them) 5.7.1 Appendix 1 \\[\\begin{align} \\phi &amp;= \\log\\frac{P(CC \\mid D)}{P(CE \\mid D)} \\\\ &amp;= \\log \\left( \\frac{\\left( \\frac{P(CC)P(D \\mid CC)}{P(D)}\\right) }{\\left( \\frac{P(CE)P(D \\mid CE)}{P(D)}\\right)} \\right) \\\\ &amp; \\textrm{Multiply numerator and denominator by $P(D)$:} \\\\ &amp;= \\log \\frac{P(CC)P(D \\mid CC)}{P(CE)P(D \\mid CE)} \\\\ &amp; \\textrm{Go from log of product to sum of logs:} \\\\ &amp;= \\log \\frac{P(CC)}{P(CE)} + \\log \\frac{P(D \\mid CC)}{P(D \\mid CE)} \\\\ &amp; \\textrm{Uniform prior over the two hypotheses, so first term becomes log of 1 (i.e., 0):} \\\\ &amp;= \\log \\frac{P(D \\mid CC)}{P(D \\mid CE)} \\\\ &amp; \\textrm{Observations are independent:} \\\\ &amp;= \\log \\prod_{t=1}^{T} \\frac{P(d_t \\mid CC)}{P(d_t \\mid CE)} \\\\ &amp; \\textrm{Log transforms product into sum:} \\\\ &amp;= \\sum_{t=1}^{T} \\log \\frac{P(d_t \\mid CC)}{P(d_t \\mid CE)} \\\\ \\end{align}\\] 5.7.2 Appendix 2: \\(\\gamma=1\\). \\[\\begin{align} P(CC) &amp;= \\frac{1}{1+e^{-\\gamma\\log \\frac{P(D \\mid CC)}{P(D \\mid CE)} }} \\\\ &amp;= \\frac{1}{1+e^{\\log \\left( \\left( \\frac{P(D \\mid CC)}{P(D \\mid CE)} \\right)^{-\\gamma} \\right) }} \\\\ &amp;= \\frac{1}{1+e^{\\log \\left( \\left( \\frac{P(D \\mid CE)}{P(D \\mid CC)} \\right)^\\gamma \\right) }} \\\\ &amp;= \\frac{1}{1+ \\left( \\frac{P(D \\mid CE)}{P(D \\mid CC)} \\right)^\\gamma} \\\\ &amp;= \\frac{1}{ 1+ \\frac{P(D \\mid CE)}{P(D \\mid CC)} } \\\\ &amp;= \\frac{1}{ \\frac{P(D \\mid CC)+P(D \\mid CE)}{P(D \\mid CC)}} \\\\ &amp;= \\frac{P(D \\mid CC)}{P(D \\mid CC)+P(D \\mid CE)} \\\\ &amp;= P(CC \\mid D) \\end{align}\\] 5.7.3 Appendix 3: Another complication The second complication in the model is the Bayesian learner might put less emphasis on older observations, i.e. they might play a smaller role in determining the posterior distribution. Recall from above that: \\[ \\phi = \\sum_{t=1}^{T} \\log \\frac{P(d_t \\mid CC)}{P(d_t \\mid CE)} \\] Now, if one of the terms of the sum is 0, then it plays no role in determining \\(\\phi\\), and therefore in the resulting probability of accepting one hypothesis or the other. Note: the term of the sum is going to be 0 when the argument of the \\(\\log\\) (the fraction) is 1, i.e. when the two hypotheses are equally probable. This makes sense, since that’s what happens when the data doesn’t privilege one hypothesis over the other. What we can do to simulate decreasing importance of information in the past is to ‘shrink’ the terms of the argument towards 0. The terms we shrink completely to 0 are equivalent to the agent disregarding the information contained in that observation. In particular, we want earlier observations to be shrunk more, as we are imagining that the participant will take more recent observations more into account. What we need is to multiply each term of the sum with a function of \\(t\\) that increases with increasing \\(t\\). Here’s one such function, where \\(\\delta\\) controls how fast the function shrinks towards zero when going towards the past: \\[ e^{-\\frac{T-t}{\\delta}} \\] Let’s plot it to make sure it has the shape we want (where \\(t\\) ranges from 1 to \\(T\\) included): T = 10 time.discount.f = function(t, T, delta){ return(exp(-(T-t)/delta)) } values.of.delta &lt;- c(0.1, 0.5, 1, 2, 4, 10) col &lt;- heat.colors(length(values.of.delta), rev=TRUE) for (i in seq_along(values.of.delta)){ curve( time.discount.f(x, T, values.of.delta[i]), from = 1, to = T, col = col[i], add = i != 1, xlab = &#39;&#39;, ylab = &#39;&#39; ) } legend(1, 1., values.of.delta, col=col, lty=1, title=expression(delta)) title( expression(&quot;Discounting factor for term t with given &quot;*delta), xlab=&#39;t&#39;, ylab=&#39;Discounting factor&#39; ) When \\(\\delta=0\\), everything but the \\(T\\)th weight is 0. When \\(\\delta=\\inf\\), every weight is 1. Therefore, we can use \\(\\delta\\) to control the ‘amount’ of rationality. Finally, the other complication. The plot shows, as the observations come in, the probability of choosing hypothesis CC. Since CC is the true hypothesis, you can see that the agent slowly becomes more and more convinced of it: gamma &lt;- 0.05 p.choosing.CC &lt;- 1/(1+exp(-gamma*phi)) plot(p.choosing.CC) Now let’s add the time discounting complication described above. delta &lt;- 50 log.proportion &lt;- log(likelihoods.data.CC/likelihoods.data.CE) phi &lt;- c() for (i in 1:n.datapoints){ time.discount &lt;- unlist(lapply( 1:i, time.discount.f, T=i, delta=delta )) new.phi &lt;- sum(log.proportion[1:i] * time.discount) phi &lt;- c(phi,new.phi) } plot(phi) "],["learning-in-a-language-of-thought.html", "6 Learning in a Language of Thought 6.1 The LogSumExp function 6.2 The conceptual primitives and number systems 6.3 Learning an LoT 6.4 If there is time left… 6.5 Exercises", " 6 Learning in a Language of Thought Reading: Piantadosi, Steven T., Joshua B. Tenenbaum, and Noah D. Goodman. “Bootstrapping in a Language of Thought: A Formal Model of Numerical Concept Learning.” Cognition 123, no. 2 (May 2012): 199–217. 6.1 The LogSumExp function Import the matrixStats library that we will use for its rowLogSumExp function. library(matrixStats) Logsumexp is an operation on a vector that allows us to sum probabilities in log-space. Effectively, it calculates the following in an intelligent way (you don’t need to worry about what happens under the hood): \\[ \\log \\left( \\sum_i \\exp(x_i) \\right) \\] In our case, we will have a vector of log-probabilities, and we will need to normalize it. To find the normalization factor, we will need the sum of the probabilities, which we will then log again and subtract from each element of the vector to normalize (recall that in log-space subtraction corresponds to division in non-log-space). We can do this sum with the LogSumExp function. 6.2 The conceptual primitives and number systems words &lt;- c(&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;eight&#39;, &#39;nine&#39;, &#39;ten&#39;) setA &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;) setB &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) In the paper, a bunch of conceptual primitives are considered, which can be combined to create an LoT. Here is how they can be written in R: singleton &lt;- function(X) length(X)==1 doubleton &lt;- function(X) length(X)==2 tripleton &lt;- function(X) length(X)==3 set.difference &lt;- function(X,Y) setdiff(X,Y) union.f &lt;- function(X,Y) union(X,Y) intersection.f &lt;- function(X,Y) intersect(X,Y) select.f &lt;- function(X) X[1] and.f &lt;- function(P,Q) P &amp; Q or.f &lt;- function(P,Q) P || Q not.f &lt;- function(P) ! Q if.f &lt;- function(P,X,Y) ifelse(P,X,Y) next.f &lt;- function(W) words[match(W, words)+1] prev.f &lt;- function(W) words[match(W, words)-1] equal.word &lt;- function(W,V) W==V We are not going to generate LoTs or sample from the grammar like they do in the paper. Rather, we are just going to use the few LoTs that are individually discussed in the paper (Figure 1). Each LoT is a function from sets to number words. Here is how they can be coded from the conceptual primitives above: oneKnower oneKnower &lt;- function(S){ return(if.f( singleton(S), &#39;one&#39;, &#39;na&#39; )) } twoKnower twoKnower &lt;- function(S){ return(if.f( singleton(S), &#39;one&#39;, if.f( doubleton(S), &#39;two&#39;, &#39;na&#39; ) )) } threeKnower threeKnower &lt;- function(S){ return(if.f( singleton(S), &#39;one&#39;, if.f( doubleton(S), &#39;two&#39;, if.f( tripleton(S), &#39;three&#39;, &#39;na&#39; ) ) )) } CPKnower CPKnower &lt;- function(S){ return(if.f( singleton(S), &#39;one&#39;, next.f( CPKnower( set.difference( S, select.f(S) ) ) ) )) } singularPlural singularPlural &lt;- function(S){ return(if.f( singleton(S), &#39;one&#39;, &#39;two&#39; )) } mod5 mod5 &lt;- function(S){ return( if.f( or.f( singleton(S), equal.word( mod5( set.difference( S, select.f(S) ) ), &#39;four&#39; ) ), &#39;one&#39;, next.f( mod5( set.difference( S, select.f(S) ) ) ) ) ) } # This one is kind of complicated, so just to convince you that it works as expected: unlist(lapply(1:10,function(n){mod5(c(1:n))})) ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;one&quot; &quot;two&quot; twoNotOneKnower twoNotOneKnower &lt;- function(S){ return(if.f( doubleton(S), &#39;two&#39;, &#39;na&#39; )) } twoNKnower twoNKnower &lt;- function(S){ if (length(S)&lt;=5){ return(if.f( singleton(S), &#39;one&#39;, next.f(next.f( twoNKnower( set.difference( S, select.f(S) ) ) )) )) } else{ return(words[length(words)]) } } LoTs &lt;- c(oneKnower, twoKnower, threeKnower, CPKnower, singularPlural, mod5, twoNotOneKnower, twoNKnower ) names(LoTs) &lt;- c(&#39;oneKnower&#39;, &#39;twoKnower&#39;, &#39;threeKnower&#39;, &#39;CPKnower&#39;, &#39;singularPlural&#39;, &#39;mod5&#39;, &#39;twoNotOneKnower&#39;, &#39;twoNKnower&#39; ) For example, let’s see what word oneKnower produces for the sets defined above: print(oneKnower(setA)) ## [1] &quot;na&quot; print(oneKnower(setB)) ## [1] &quot;na&quot; 6.3 Learning an LoT What we are really interested in is the likelihood function, i.e. the function from a set (really, a set size) to the probability of producing each number word, in each LoT. The authors add two complications. First, when ‘NA’ is produced, a word is chosen uniformly from the number words. Second, with a small probability the speaker chooses a number word at random. Let’s write a function that for a given LoT gives the probability of each number word given a set size. word.to.p &lt;- function(proposed.word, literal.word, alpha = 0.95) { # equation 5 in the paper if (literal.word == &#39;na&#39;) { return(1 / length(words)) } else if (literal.word == proposed.word) { alpha + (1 - alpha) * (1 / length(words)) } else { (1 - alpha) * (1 / length(words)) } } likelihood.f &lt;- function(LoT, set.to.evaluate, alpha = 0.7) { word &lt;- LoT(set.to.evaluate) list.p &lt;- unlist(lapply( words, word.to.p, literal.word = word, alpha = alpha )) names(list.p) &lt;- words return(list.p) } Let’s test the likelihood function by producing the probability of each number word produced by each LoT given setA: lapply( LoTs, likelihood.f, set.to.evaluate = setA, alpha = 0.9 ) ## $oneKnower ## one two three four five six seven eight nine ten ## 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ## ## $twoKnower ## one two three four five six seven eight nine ten ## 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ## ## $threeKnower ## one two three four five six seven eight nine ten ## 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ## ## $CPKnower ## one two three four five six seven eight nine ten ## 0.01 0.01 0.01 0.01 0.91 0.01 0.01 0.01 0.01 0.01 ## ## $singularPlural ## one two three four five six seven eight nine ten ## 0.01 0.91 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ## ## $mod5 ## one two three four five six seven eight nine ten ## 0.91 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ## ## $twoNotOneKnower ## one two three four five six seven eight nine ten ## 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ## ## $twoNKnower ## one two three four five six seven eight nine ten ## 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.91 0.01 Function likelihood.f gives us the probability of each number word, given an LoT and a set. Formally, \\(P(w | t, c, L)\\) for each possible value of \\(w\\), given specific values of \\(t,c,L\\). The prior is calculated in a rather complex way as a function of the length of the expression encoding the hypothesis (the number system) and whether it contains recursion (i.e. whether the function calls itself). We could try and write the code to do this based on derivation lengths, but it would be pretty complicated. Let’s just eyeball it: alpha &lt;- 3 # I am adding 2 for L, the recursion n.operators.used &lt;- c(2, 4, 6, 6+2, 2, 9+2*2, 2, 6+2) names(n.operators.used) &lt;- names(LoTs) unnorm.softmax &lt;- exp(alpha*-n.operators.used) priors &lt;- c(unnorm.softmax) / sum(unnorm.softmax) priors ## oneKnower twoKnower threeKnower CPKnower singularPlural mod5 ## 3.330575e-01 8.255669e-04 2.046376e-06 5.072458e-09 3.330575e-01 1.551677e-15 ## twoNotOneKnower twoNKnower ## 3.330575e-01 5.072458e-09 What we want is \\(P(L | W, T, C)\\). We can apply Bayes rule: \\[\\begin{align} P(L | W, T, C) &amp;= \\frac{P(W | L, T, C) P(L | T,C)}{P(W | T, C)} \\\\ &amp;= \\frac{P(W | L, T, C) P(L)}{P(W | T, C)} \\\\ &amp;= P(L)\\frac{\\prod_i P(w_i | L, t_i, c_i)}{\\sum_i P(W |l_i, T, C) P(l_i)} \\\\ &amp;= P(L)\\frac{\\prod_i P(w_i | L, t_i, c_i)}{\\sum_i \\left( \\prod_j P(w_j |l_i, t_j, c_j) P(l_i) \\right)} \\\\ \\end{align}\\] So all the indices vary over dimensions that we can calculate explicitly: either the observations, or the LoTs. In particular, in the original papers they can’t simply loop over the LoTs, since they consider the infinite space of all LoTs, and that forces them to do something fancier called MCMC. However, we only consider the handful of LoTs defined above. n.elements.to.set takes a number and returns a set with the given number of elements. The elements of the set are shuffled letters. n.elements.to.set &lt;- function(n.elements){ return(sample(c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;,&#39;j&#39;))[1:n.elements]) } probs gives the probability of each numeral (eyeballing Figure 2 from the paper). probs &lt;- c(0.7, 0.14, 0.05, 0.025, 0.02, 0.018, 0.012, 0.02, 0.01, 0.005) # print(1-sum(probs)) barplot(probs) Specify the number of observations to show the learner, create vector of set sizes. n.observations &lt;- 350 set.sizes &lt;- sample( 10, n.observations, replace = TRUE, prob=probs ) Create a vector of sets with the specified sizes. observed.sets &lt;- lapply(set.sizes, n.elements.to.set) Create vector of true words using the CPKnower: true.words &lt;- unlist(lapply(observed.sets, CPKnower)) NOTE: in the paper, they assume these words are produced noisily, while here we just produce them without noise. For each LoT and each set, get proposed word for that set in that LoT: proposed.words &lt;- data.frame(row.names = c(1:n.observations)) for (lot.name in names(LoTs)){ new.words &lt;- unlist(lapply(observed.sets, LoTs[[lot.name]])) proposed.words[[lot.name]] &lt;- new.words } For each LoT, calculate the likelihood of each word given each set: loglikelihoods &lt;- data.frame(row.names=c(1:n.observations)) for (i in colnames(proposed.words)){ # print(proposed.words[i]) p.lot &lt;- mapply(word.to.p, c(proposed.words[[i]]), true.words) loglikelihoods[i] &lt;- log(p.lot) } tail(loglikelihoods) ## oneKnower twoKnower threeKnower CPKnower singularPlural mod5 twoNotOneKnower ## 345 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -5.29831737 ## 346 -5.29831737 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 ## 347 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -5.29831737 ## 348 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -5.29831737 ## 349 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -5.29831737 ## 350 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -0.04604394 -5.29831737 ## twoNKnower ## 345 -0.04604394 ## 346 -5.29831737 ## 347 -0.04604394 ## 348 -0.04604394 ## 349 -0.04604394 ## 350 -0.04604394 NOTE: the CPKnower always gets the highest possible likelihood for all true number words, because it’s the true speaker! The cumulative product of the likelihood (or in this case the cumulative sum of the loglikelihood) tells us what the likelihood was at each successive observation for each LoT. cumulative.loglikelihood &lt;- as.matrix(sapply(loglikelihoods, cumsum)) unnormalized.cumulative.logposterior &lt;- cumulative.loglikelihood + log(priors) Lognormalize the result: posterior &lt;- exp( unnormalized.cumulative.logposterior - matrixStats::rowLogSumExps(unnormalized.cumulative.logposterior) ) head(posterior) ## oneKnower twoKnower threeKnower CPKnower singularPlural mod5 twoNotOneKnower ## [1,] 5.102010e-03 5.102010e-03 5.102010e-03 5.987436e-06 5.102010e-03 9.744839e-01 5.102010e-03 ## [2,] 4.994109e-01 3.068486e-06 9.386571e-13 5.860809e-04 4.994109e-01 5.860809e-04 4.914435e-15 ## [3,] 1.595899e-08 2.597402e-03 2.597402e-03 4.961038e-01 1.595899e-08 4.961038e-01 7.119877e-08 ## [4,] 3.200110e-08 5.208333e-03 3.200110e-08 1.869739e-12 3.200110e-08 9.947916e-01 4.592666e-15 ## [5,] 5.128174e-03 3.150859e-08 5.128174e-03 9.794813e-01 5.128174e-03 6.018141e-06 3.853269e-12 ## [6,] 9.817362e-15 6.129787e-06 9.976523e-01 1.170789e-03 1.875116e-12 1.170789e-03 7.496279e-10 ## twoNKnower ## [1,] 3.134783e-08 ## [2,] 3.068486e-06 ## [3,] 2.597402e-03 ## [4,] 9.789212e-15 ## [5,] 5.128174e-03 ## [6,] 3.209313e-08 Finally, let’s plot the posterior distribution. You can see that the general pattern from the paper holds: initially, various other hypotheses dominate, but eventually over enough observations the CPKnower wins out. Note that this plot is similar but not identical to Figure 3(a) in the paper. The difference is that they consider all possible LoTs, and therefore more than one LoT encodes each type of agent - e.g. more than one LoT is a oneKnower etc. On the other hand, we specified a small list of LoTs. posterior &lt;- as.data.frame(posterior) matplot( rownames(posterior), posterior, type = &#39;l&#39;, xlab = &#39;Observation&#39;, ylab = &#39;Posterior&#39;, col = 1:ncol(posterior) ) legend( &#39;bottomright&#39;, inset = .05, legend = colnames(posterior), pch = 1, col = 1:ncol(posterior) ) 6.4 If there is time left… Put the above code into a function that runs the simulation repeatedly, and check what happens when you average multiple runs of the simulation. Come up with a different LoT and add it to the list above, and run the simulation. How does it compare to the other LoTs? Add noise to the generation of the observed number words. How does it change the results? 6.5 Exercises Exercise 6.1 Change the probabilities of seeing the numerals (in probs) so that the agent only ever sees ‘one’ and ‘two’, with equal probability. Train the agent again, plot the result, and describe what happens to the learning. Exercise 6.2 Use the conceptual primitives defined in the notebook to define an LoT "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
